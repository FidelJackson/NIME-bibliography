@inproceedings{NIME21_1,
  article-number = {1},
  author = {Fasciani, Stefano and Goode, Jackson},
  title = {20 NIMEs: Twenty Years of New Interfaces for Musical Expression},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2021},
  month = jun,
  address = {Shanghai, China},
  issn = {2220-4806},
  doi = {10.21428/92fbeb44.b368bcd5},
  url = {https://nime.pubpub.org/pub/20nimes},
  presentation-video = {https://youtu.be/44W7dB7lzQg},
  abstract = {This paper provides figures and metrics over twenty years of New Interfaces for Musical Expression conferences, which are derived by analyzing the publicly available paper proceedings. Besides presenting statistical information and a bibliometric study, we aim at identifying trends and patterns. The analysis shows the growth and heterogeneity of the NIME demographic, as well the increase in research output. The data presented in this paper allows the community to reflect on several issues such as diversity and sustainability, and it provides insights to address challenges and set future directions.}
}

@inproceedings{NIME21_2,
  article-number = {2},
  author = {Arbel, Lior},
  title = {Aeolis: A Virtual Instrument Producing Pitched Tones With Soundscape Timbres},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2021},
  month = jun,
  address = {Shanghai, China},
  issn = {2220-4806},
  doi = {10.21428/92fbeb44.64f66047},
  url = {https://nime.pubpub.org/pub/c3w33wya},
  presentation-video = {https://youtu.be/C0WEeaYy0tQ},
  abstract = {Ambient sounds such as breaking waves or rustling leaves are sometimes used in music recording, composition and performance. However, as these sounds lack a precise pitch, they can not be used melodically. This work describes Aeolis, a virtual instrument producing pitched tones from a real-time ambient sound input using subtractive synthesis. The produced tones retain the identifiable timbres of the ambient sounds. Tones generated using input sounds from various environments, such as sea waves, leaves rustle and traffic noise, are analyzed. A configuration for a live in-situ performance is described, consisting of live streaming the produced sounds. In this configuration, the environment itself acts as a ‘performer’ of sorts, alongside the Aeolis player, providing both real-time input signals and complementary visual cues.}
}

@inproceedings{NIME21_3,
  article-number = {3},
  author = {Förster, Andreas and Komesker, Mathias},
  title = {LoopBlocks: Design and Preliminary Evaluation of an Accessible Tangible Musical Step Sequencer},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2021},
  month = jun,
  address = {Shanghai, China},
  issn = {2220-4806},
  doi = {10.21428/92fbeb44.f45e1caf},
  url = {https://nime.pubpub.org/pub/bj2w1gdx},
  presentation-video = {https://youtu.be/u5o0gmB3MX8},
  abstract = {This paper presents the design and preliminary evaluation of an Accessible Digital Musical Instrument (ADMI) in the form of a tangible wooden step sequencer that uses photoresistors and wooden blocks to trigger musical events. Furthermore, the paper presents a short overview of design criteria for ADMIs based on literature and first insights of an ongoing qualitative interview study with German Special Educational Needs (SEN) teachers conducted by the first author. The preliminary evaluation is realized by a reflection on the mentioned criteria. The instrument was designed as a starting point for a participatory design process in music education settings. The software is programmed in Pure Data and running on a Raspberry Pi computer that fits inside the body of the instrument. While most similar developments focus on professional performance and complex interactions, LoopBlocks focuses on accessibility and Special Educational Needs settings. The main goal is to reduce the cognitive load needed to play music by providing a clear and constrained interaction, thus reducing intellectual and technical barriers to active music making.}
}

@inproceedings{NIME21_4,
  article-number = {4},
  author = {Calegario, Filipe and Tragtenberg, João and Frisson, Christian and Meneses, Eduardo and Malloch, Joseph and Cusson, Vincent and Wanderley, Marcelo M.},
  title = {Documentation and Replicability in the NIME Community},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2021},
  month = jun,
  address = {Shanghai, China},
  issn = {2220-4806},
  doi = {10.21428/92fbeb44.dc50e34d},
  url = {https://nime.pubpub.org/pub/czq0nt9i},
  presentation-video = {https://youtu.be/ySh5SueLMAA},
  abstract = {In this paper, we discuss the importance of replicability in Digital Musical Instrument (DMI) design and the NIME community. Replication enables us to: create new artifacts based on existing ones, experiment DMIs in different contexts and cultures, and validate obtained results from evaluations. We investigate how the papers present artifact documentation and source code by analyzing the NIME proceedings from 2018, 2019, and 2020. We argue that the presence and the quality of documentation are good indicators of replicability and can be beneficial for the NIME community. Finally, we discuss the importance of documentation for replication, propose a call to action towards more replicable projects, and present a practical guide informing future steps toward replicability in the NIME community.}
}

@inproceedings{NIME21_5,
  article-number = {5},
  author = {Koutsomichalis, Marinos},
  title = {A Yellow Box with a Key Switch and a 1/4" TRS Balanced Audio Output},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2021},
  month = jun,
  address = {Shanghai, China},
  issn = {2220-4806},
  doi = {10.21428/92fbeb44.765a94a7},
  url = {https://nime.pubpub.org/pub/n69uznd4},
  presentation-video = {https://youtu.be/_IUT0tbtkBI},
  abstract = {This short article presents a reductionist infra-instrument. It concerns a yellow die-cast aluminium box only featuring a key switch and a 1/4” TRS balanced audio output as its UI. On the turn of the key, the device performs a certain poem in Morse code and via very low frequency acoustic pulses; in this way, it transforms poetry into bursts of intense acoustic energy that may resonate a hosting architecture and any human bodies therein. It is argued that the instrument functions at the very same time as a critical/speculative electronic object, as an ad-hoc performance instrument, and as a piece of (conceptual) art on its own sake.}
}

@inproceedings{NIME21_6,
  article-number = {6},
  author = {Gillick, Jon and Bamman, David},
  title = {What to Play and How to Play it: Guiding Generative Music Models with Multiple Demonstrations},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2021},
  month = jun,
  address = {Shanghai, China},
  issn = {2220-4806},
  doi = {10.21428/92fbeb44.06e2d5f4},
  url = {https://nime.pubpub.org/pub/s3x60926},
  presentation-video = {https://youtu.be/Q2M_smiN6oo},
  abstract = {We propose and evaluate an approach to incorporating multiple user-provided inputs, each demonstrating a complementary set of musical characteristics, to guide the output of a generative model for synthesizing short music performances or loops. We focus on user inputs that describe both “what to play” (via scores in MIDI format) and “how to play it” (via rhythmic inputs to specify expressive timing and dynamics). Through experiments, we demonstrate that our method can facilitate human-AI co-creation of drum loops with diverse and customizable outputs. In the process, we argue for the interaction paradigm of mapping by demonstration as a promising approach to working with deep learning models that are capable of generating complex and realistic musical parts.}
}

@inproceedings{NIME21_7,
  article-number = {7},
  author = {Dunham, Paul and Zareei, Dr. Mo H. and Carnegie, Prof. Dale and McKinnon, Dr. Dugal},
  title = {Click::RAND#2. An Indeterminate Sound Sculpture},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2021},
  month = jun,
  address = {Shanghai, China},
  issn = {2220-4806},
  doi = {10.21428/92fbeb44.5cc6d157},
  url = {https://nime.pubpub.org/pub/lac4s48h},
  presentation-video = {https://youtu.be/vJynbs8txuA},
  abstract = {Can random digit data be transformed and utilized as a sound installation that provides a referential connection between a book and the electromechanical computer? What happens when the text of A Million Random Digits with 100,000 Normal Deviates is ‘vocalized’ by an electro-mechanical object? Using a media archaeological research approach, Click::RAND^(#)2, an indeterminate sound sculpture utilising relays as sound objects, is an audio-visual reinterpretation and representation of an historical relationship between a book of random digits and the electromechanical relay. Developed by the first author, Click::RAND^(#)2 is the physical re-presentation of random digit data sets as compositional elements to complement the physical presence of the work through spatialized sound patterns framed within the context of Henri Lefebvre’s rhythmanalysis and experienced as synchronous, syncopated or discordant rhythms.}
}

@inproceedings{NIME21_8,
  article-number = {8},
  author = {Bin, S. M. Astrid},
  title = {Discourse is critical: Towards a collaborative NIME history},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2021},
  month = jun,
  address = {Shanghai, China},
  issn = {2220-4806},
  doi = {10.21428/92fbeb44.ac5d43e1},
  url = {https://nime.pubpub.org/pub/nbrrk8ll},
  presentation-video = {https://youtu.be/omnMRlj7miA},
  abstract = {Recent work in NIME has questioned the political and social implications of work in this field, and has called for direct action on problems in the areas of diversity, representation and political engagement. Though there is motivation to address these problems, there is an open question of how to meaningfully do so. This paper proposes that NIME’s historical record is the best tool for understanding our own output but this record is incomplete, and makes the case for collective action to improve how we document our work. I begin by contrasting NIME’s output with its discourse, and explore the nature of this discourse through NIME history and examine our inherited epistemological complexity. I assert that, if left unexamined, this complexity can undermine our community values of diversity and inclusion. I argue that meaningfully addressing current problems demands critical reflection on our work, and explore how NIME’s historical record is currently used as a means of doing so. I then review what NIME's historical record contains (and what it does not), and evaluate its fitness for use as a tool of inquiry. Finally I make the case for collective action to establish better documentation practices, and suggest features that may be helpful for the process as well as the result.}
}

@inproceedings{NIME21_9,
  article-number = {9},
  author = {Leigh, Sang-won and Lee, Jeonghyun (Jonna)},
  title = {A Study on Learning Advanced Skills on Co-Playable Robotic Instruments},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2021},
  month = jun,
  address = {Shanghai, China},
  issn = {2220-4806},
  doi = {10.21428/92fbeb44.002be215},
  url = {https://nime.pubpub.org/pub/h5dqsvpm},
  presentation-video = {https://youtu.be/MeXrN95jajU},
  abstract = {Learning advanced skills on a musical instrument takes a range of physical and cognitive efforts. For instance, practicing polyrhythm is a complex task that requires the development of both musical and physical skills. This paper explores the use of automation in the context of learning advanced skills on the guitar. Our robotic guitar is capable of physically plucking on the strings along with a musician, providing both haptic and audio guidance to the musician. We hypothesize that a multimodal and first-person experience of “being able to play” could increase learning efficacy. We discuss the novel learning application and a user study, through which we illustrate the implication and potential issues in systems that provide temporary skills and in-situ multimodal guidance for learning.}
}

@inproceedings{NIME21_10,
  article-number = {10},
  author = {Masu, Raul and Correia, Nuno N. and Romao, Teresa},
  title = {NIME Scores: a Systematic Review of How Scores Have Shaped Performance Ecologies in NIME},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2021},
  month = jun,
  address = {Shanghai, China},
  issn = {2220-4806},
  doi = {10.21428/92fbeb44.3ffad95a},
  url = {https://nime.pubpub.org/pub/41cj1pyt},
  presentation-video = {https://youtu.be/j7XmQvDdUPk},
  abstract = {This paper investigates how the concept of score has been used in the NIME community. To this end, we performed a systematic literature review of the NIME proceedings, analyzing papers in which scores play a central role. We analyzed the score not as an object per se but in relation to the users and the interactive system(s). In other words, we primarily looked at the role that scores play in the performance ecology. For this reason, to analyze the papers, we relied on ARCAA, a recent framework created to investigate artifact ecologies in computer music performances. Using the framework, we created a scheme for each paper and clustered the papers according to similarities. Our analysis produced five main categories that we present and discuss in relation to literature about musical scores.}
}

@inproceedings{NIME21_11,
  article-number = {11},
  author = {Frisson, Christian and Bredholt, Mathias and Malloch, Joseph and Wanderley, Marcelo M.},
  title = {MapLooper: Live-looping of distributed gesture-to-sound mappings},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2021},
  month = jun,
  address = {Shanghai, China},
  issn = {2220-4806},
  doi = {10.21428/92fbeb44.47175201},
  url = {https://nime.pubpub.org/pub/2pqbusk7},
  presentation-video = {https://youtu.be/9r0zDJA8qbs},
  abstract = {This paper presents the development of MapLooper: a live-looping system for gesture-to-sound mappings. We first reviewed loop-based Digital Musical Instruments (DMIs). We then developed a connectivity infrastructure for wireless embedded musical instruments with distributed mapping and synchronization. We evaluated our infrastructure in the context of the real-time constraints of music performance. We measured a round-trip latency of 4.81 ms when mapping signals at 100 Hz with embedded libmapper and an average inter-onset delay of 3.03 ms for synchronizing with Ableton Link. On top of this infrastructure, we developed MapLooper: a live-looping tool with 2 example musical applications: a harp synthesizer with SuperCollider and embedded source-filter synthesis with FAUST on ESP32. Our system is based on a novel approach to mapping, extrapolating from using FIR and IIR filters on gestural data to using delay-lines as part of the mapping of DMIs. Our system features rhythmic time quantization and a flexible loop manipulation system for creative musical exploration. We open-source all of our components.}
}

@inproceedings{NIME21_12,
  article-number = {12},
  author = {Reimer, P. J. Charles and Wanderley, Marcelo M.},
  title = {Embracing Less Common Evaluation Strategies for Studying User Experience in NIME},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2021},
  month = jun,
  address = {Shanghai, China},
  issn = {2220-4806},
  doi = {10.21428/92fbeb44.807a000f},
  url = {https://nime.pubpub.org/pub/fidgs435},
  presentation-video = {https://youtu.be/WTaee8NVtPg},
  abstract = {Assessment of user experience (UX) is increasingly important in music interaction evaluation, as witnessed in previous NIME reviews describing varied and idiosyncratic evaluation strategies. This paper focuses on evaluations conducted in the last four years of NIME (2017 to 2020), compares results to previous research, and classifies evaluation types to describe how researchers approach and study UX in NIME. While results of this review confirm patterns such as the prominence of short-term, performer perspective evaluations, and the variety of evaluation strategies used, they also show that UX-focused evaluations are typically exploratory and limited to novice performers. Overall, these patterns indicate that current UX evaluation strategies do not address dynamic factors such as skill development, the evolution of the performer-instrument relationship, and hedonic and cognitive aspects of UX. To address such limitations, we discuss a number of less common tools developed within and outside of NIME that focus on dynamic aspects of UX, potentially leading to more informative and meaningful evaluation insights.}
}

@inproceedings{NIME21_13,
  article-number = {13},
  author = {Fukuda, Takuto and Meneses, Eduardo and West, Travis and Wanderley, Marcelo M.},
  title = {The T-Stick Music Creation Project: An approach to building a creative community around a DMI},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2021},
  month = jun,
  address = {Shanghai, China},
  issn = {2220-4806},
  doi = {10.21428/92fbeb44.26f33210},
  url = {https://nime.pubpub.org/pub/7c4qdj4u},
  presentation-video = {https://youtu.be/tfOUMr3p4b4},
  abstract = {To tackle digital musical instrument (DMI) longevity and the problem of the second performer, we proposed the T-Stick Music Creation Project, a series of musical commissions along with workshops, mentorship, and technical support, meant to foment composition and performance using the T-Stick and provide an opportunity to improve technical and pedagogical support for the instrument. Based on the project’s outcomes, we describe three main contributions: our approach; the artistic works produced; and analysis of these works demonstrating the T-Stick as actuator, modulator, and data provider.}
}

@inproceedings{NIME21_14,
  article-number = {14},
  author = {Cavdir, Doga and Clarke, Chris and Chiu, Patrick and Denoue, Laurent and Kimber, Don},
  title = {Reactive Video: Movement Sonification for Learning Physical Activity with Adaptive Video Playback},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2021},
  month = jun,
  address = {Shanghai, China},
  issn = {2220-4806},
  doi = {10.21428/92fbeb44.eef53755},
  url = {https://nime.pubpub.org/pub/dzlsifz6},
  presentation-video = {https://youtu.be/pbvZI80XgEU},
  abstract = {This paper provides initial efforts in developing and evaluating a real-time movement sonification framework for physical activity practice and learning. Reactive Video provides an interactive, vision-based, adaptive video playback with auditory feedback on users' performance to better support when learning and practicing new physical skills. We implement the sonification for auditory feedback design by extending the Web Audio API framework. The current application focuses on Tai-Chi performance and provides two main audio cues to users for several Tai Chi exercises. We provide our design approach, implementation, and sound generation and mapping, specifically for interactive systems with direct video manipulation. Our observations reveal the relationship between the movement-to-sound mapping and characteristics of the physical activity.}
}

@inproceedings{NIME21_15,
  article-number = {15},
  author = {Chin, Daniel and Zhang, Ian and Xia, Gus},
  title = {Hyper-hybrid Flute: Simulating and Augmenting How Breath Affects Octave and Microtone},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2021},
  month = jun,
  address = {Shanghai, China},
  issn = {2220-4806},
  doi = {10.21428/92fbeb44.c09d91be},
  url = {https://nime.pubpub.org/pub/eshr},
  presentation-video = {https://youtu.be/UIqsYK9F4xo},
  abstract = {We present hyper-hybrid flute, a new interface which can be toggled between its electronic mode and its acoustic mode. In its acoustic mode, the interface is identical to the regular six-hole recorder. In its electronic mode, the interface detects the player's fingering and breath velocity and translates them to MIDI messages. Specifically, it maps higher breath velocity to higher octaves, with the modulo remainder controlling the microtonal pitch bend. This novel mapping reproduces a highly realistic flute-playing experience. Furthermore, changing the parameters easily augments the interface into a hyperinstrument that allows the player to control microtones more expressively via breathing techniques.}
}

@inproceedings{NIME21_16,
  article-number = {16},
  author = {Rossmy, Beat and Wiethoff, Alexander},
  title = {Musical Grid Interfaces: Past, Present, and Future Directions},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2021},
  month = jun,
  address = {Shanghai, China},
  issn = {2220-4806},
  doi = {10.21428/92fbeb44.6a2451e6},
  url = {https://nime.pubpub.org/pub/grid-past-present-future},
  presentation-video = {https://youtu.be/GuPIz2boJwA},
  abstract = {This paper examines grid interfaces which are currently used in many musical devices and instruments. This type of interface concept has been rooted in the NIME community since the early 2000s. We provide an overview of research projects and commercial products and conducted an expert interview as well as an online survey. In summary this work shares: (1) an overview on grid controller research, (2) a set of three usability issues deduced by a multi method approach, and (3) an evaluation of user perceptions regarding persistent usability issues and common reasons for the use of grid interfaces.}
}

@inproceedings{NIME21_17,
  article-number = {17},
  author = {Rossmy, Beat and Unger, Sebastian and Wiethoff, Alexander},
  title = {TouchGrid – Combining Touch Interaction with Musical Grid Interfaces},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2021},
  month = jun,
  address = {Shanghai, China},
  issn = {2220-4806},
  doi = {10.21428/92fbeb44.303223db},
  url = {https://nime.pubpub.org/pub/touchgrid},
  presentation-video = {https://youtu.be/ti2h_WK5NeU},
  abstract = {Musical grid interfaces such as the monome grid have developed into standard interfaces for musical equipment over the last 15 years. However, the types of possible interactions more or less remained the same, only expanding grid capabilities by external IO elements. Therefore, we propose to transfer capacitive touch technology to grid devices to expand their input capabilities by combining tangible and capacitive-touch based interaction paradigms. This enables to keep the generic nature of grid interfaces which is a key feature for many users. In this paper we present the TouchGrid concept and share our proof-of-concept implementation as well as an expert evaluation regarding the general concept of touch interaction used on grid devices. TouchGrid provides swipe and bezel interaction derived from smart phone interfaces to allow navigation between applications and access to menu systems in a familiar way.}
}

@inproceedings{NIME21_18,
  article-number = {18},
  author = {Ford, Corey and Bryan-Kinns, Nick and Nash, Chris},
  title = {Creativity in Children's Digital Music Composition},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2021},
  month = jun,
  address = {Shanghai, China},
  issn = {2220-4806},
  doi = {10.21428/92fbeb44.e83deee9},
  url = {https://nime.pubpub.org/pub/ker5w948},
  presentation-video = {https://youtu.be/XpMiDWrxXMU},
  abstract = {Composing is a neglected area of music education. To increase participation, many technologies provide open-ended interfaces to motivate child autodidactic use, drawing influence from Papert’s LOGO philosophy to support children’s learning through play. This paper presents a case study examining which interactions with Codetta, a LOGO-inspired, block-based music platform, supports children’s creativity in music composition. Interaction logs were collected from 20 children and correlated against socially-validated creativity scores. To conclude, we recommend that the transition between low-level edits and high-level processes should be carefully scaffolded.}
}

@inproceedings{NIME21_19,
  article-number = {19},
  author = {Li, Yinmiao and Piao, Ziyue and Xia, Gus},
  title = {A Wearable Haptic Interface for Breath Guidance in Vocal Training},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2021},
  month = jun,
  address = {Shanghai, China},
  issn = {2220-4806},
  doi = {10.21428/92fbeb44.6d342615},
  url = {https://nime.pubpub.org/pub/cgi7t0ta},
  presentation-video = {https://youtu.be/-t-u0V-27ng},
  abstract = {Various studies have shown that haptic interfaces could enhance the learning efficiency in music learning, but most existing studies focus on training motor skills of instrument playing such as finger motions. In this paper, we present a wearable haptic device to guide diaphragmatic breathing, which can be used in vocal training as well as the learning of wind instruments. The device is a wearable strap vest, consisting of a spinal exoskeleton on the back for inhalation and an elastic belt around the waist for exhalation. We first conducted case studies to assess how convenient and comfortable to wear the device, and then evaluate its effectiveness in guiding rhythm and breath. Results show users' acceptance of the haptic interface and the potential of haptic guidance in vocal training.}
}

@inproceedings{NIME21_20,
  article-number = {20},
  author = {Berthaut, Florent},
  title = {Musical Exploration of Volumetric Textures in Mixed and Virtual Reality},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2021},
  month = jun,
  address = {Shanghai, China},
  issn = {2220-4806},
  doi = {10.21428/92fbeb44.6607d04f},
  url = {https://nime.pubpub.org/pub/sqceyucq},
  presentation-video = {https://youtu.be/C9EiA3TSUag},
  abstract = {The development of technologies for acquisition and display gives access to a large variety of volumetric (3D) textures, either synthetic or obtained through tomography. They constitute extremely rich data which is usually explored for informative purposes, in medical or engineering contexts. We believe that this exploration has a strong potential for musical expression. To that extent, we propose a design space for the musical exploration of volumetric textures. We describe the challenges for its implementation in Virtual and Mixed-Reality and we present a case study with an instrument called the Volume Sequencer which we analyse using your design space. Finally, we evaluate the impact on expressive exploration of two dimensions, namely the amount of visual feedback and the selection variability.}
}

@inproceedings{NIME21_21,
  article-number = {21},
  author = {Aresty, Abby and Gibson, Rachel},
  title = {Changing GEAR: The Girls Electronic Arts Retreat's Teaching Interfaces for Musical Expression},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2021},
  month = jun,
  address = {Shanghai, China},
  issn = {2220-4806},
  doi = {10.21428/92fbeb44.25757aca},
  url = {https://nime.pubpub.org/pub/8lop0zj4},
  presentation-video = {https://youtu.be/8qeFjNGaEHc},
  abstract = {The Girls Electronic Arts Retreat (GEAR) is a STEAM summer camp for ages 8 - 11. In this paper, we compare and contrast lessons from the first two iterations of GEAR, including one in-person and one remote session. We introduce our Teaching Interfaces for Musical Expression (TIME) framework and use our analyses to compose a list of best practices in TIME development and implementation.}
}

@inproceedings{NIME21_22,
  article-number = {22},
  author = {Andersen, Anne Sophie and Kwan, Derek},
  title = {Grisey’s 'Talea': Musical Representation As An Interactive 3D Map},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2021},
  month = jun,
  address = {Shanghai, China},
  issn = {2220-4806},
  doi = {10.21428/92fbeb44.27d09832},
  url = {https://nime.pubpub.org/pub/oiwz8bb7},
  presentation-video = {https://youtu.be/PGYOkFjyrek},
  abstract = {The praxis of using detailed visual models to illustrate complex ideas is widely used in the sciences but less so in music theory. Taking the composer’s notes as a starting point, we have developed a complete interactive 3D model of Grisey’s Talea (1986). Our model presents a novel approach to music education and theory by making understanding of complex musical structures accessible to students and non-musicians, particularly those who struggle with traditional means of learning or whose mode of learning is predominantly visual. The model builds on the foundations of 1) the historical associations between visual and musical arts and those concerning spectralists in particular 2) evidence of recurring cross-modal associations in the general population and consistent associations for individual synesthetes. Research into educational uses of the model is a topic for future exploration.}
}

@inproceedings{NIME21_23,
  article-number = {23},
  author = {Tomás, Enrique and Gorbach, Thomas and Tellioğlu, Hilda and Kaltenbrunner, Martin},
  title = {Embodied Gestures: Sculpting Energy-Motion Models into Musical Interfaces},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2021},
  month = jun,
  address = {Shanghai, China},
  issn = {2220-4806},
  doi = {10.21428/92fbeb44.ce8139a8},
  url = {https://nime.pubpub.org/pub/gsx1wqt5},
  presentation-video = {https://youtu.be/QDjCEnGYSC4},
  abstract = {In this paper we discuss the beneficial aspects of incorporating energy-motion models as a design pattern in musical interface design. These models can be understood as archetypes of motion trajectories which are commonly applied in the analysis and composition of acousmatic music. With the aim of exploring a new possible paradigm for interface design, our research builds on the parallel investigation of embodied music cognition theory and the praxis of acousmatic music. After having run a large study for understanding a listener’s spontaneous rendering of form and movement, we built a number of digital instruments especially designed to emphasise a particular energy-motion profile. The evaluation through composition and performance indicates that this design paradigm can foster musical inventiveness and expression in the processes of composition and performance of gestural electronic music.}
}

@inproceedings{NIME21_24,
  article-number = {24},
  author = {Masu, Raul and Melbye, Adam Pultz and Sullivan, John and Jensenius, Alexander Refsum},
  title = {NIME and the Environment: Toward a More Sustainable NIME Practice},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2021},
  month = jun,
  address = {Shanghai, China},
  issn = {2220-4806},
  doi = {10.21428/92fbeb44.5725ad8f},
  url = {https://nime.pubpub.org/pub/4bbl5lod},
  presentation-video = {https://youtu.be/JE6YqYsV5Oo},
  abstract = {This paper addresses environmental issues around NIME research and practice. We discuss the formulation of an environmental statement for the conference as well as the initiation of a NIME Eco Wiki containing information on environmental concerns related to the creation of new musical instruments. We outline a number of these concerns and, by systematically reviewing the proceedings of all previous NIME conferences, identify a general lack of reflection on the environmental impact of the research undertaken. Finally, we propose a framework for addressing the making, testing, using, and disposal of NIMEs in the hope that sustainability may become a central concern to researchers.}
}

@inproceedings{NIME21_25,
  article-number = {25},
  author = {Harlow, Randall and Petersson, Mattias and Ek, Robert and Visi, Federico and Östersjö, Stefan},
  title = {Global Hyperorgan: a platform for telematic musicking and research},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2021},
  month = jun,
  address = {Shanghai, China},
  issn = {2220-4806},
  doi = {10.21428/92fbeb44.d4146b2d},
  url = {https://nime.pubpub.org/pub/a626cbqh},
  presentation-video = {https://youtu.be/t88aIXdqBWQ},
  abstract = {The Global Hyperorgan is an intercontinental, creative space for acoustic musicking. Existing pipe organs around the world are networked for real-time, geographically-distant performance, with performers utilizing instruments and other input devices to collaborate musically through the voices of the pipes in each location. A pilot study was carried out in January 2021, connecting two large pipe organs in Piteå, Sweden, and Amsterdam, the Netherlands. A quartet of performers tested the Global Hyperorgan’s capacities for telematic musicking through a series of pieces. The concept of modularity is useful when considering the artistic challenges and possibilities of the Global Hyperorgan. We observe how the modular system utilized in the pilot study afforded multiple experiences of shared instrumentality from which new, synthetic voices emerge. As a long-term technological, artistic and social research project, the Global Hyperorgan offers a platform for exploring technology, agency, voice, and intersubjectivity in hyper-acoustic telematic musicking.}
}

@inproceedings{NIME21_26,
  article-number = {26},
  author = {Zayas-Garin, Luis and Harrison, Jacob and Jack, Robert and McPherson, Andrew},
  title = {DMI Apprenticeship: Sharing and Replicating Musical Artefacts},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2021},
  month = jun,
  address = {Shanghai, China},
  issn = {2220-4806},
  doi = {10.21428/92fbeb44.87f1d63e},
  url = {https://nime.pubpub.org/pub/dmiapprenticeship},
  presentation-video = {https://youtu.be/zTMaubJjlzA},
  abstract = {The nature of digital musical instruments (DMIs), often bespoke artefacts devised by single or small groups of technologists, requires thought about how they are shared and archived so that others can replicate or adapt designs. The ability for replication contributes to an instrument’s longevity and creates opportunities for both DMI designers and researchers. Research papers often omit necessary knowledge for replicating research artefacts, but we argue that mitigating this situation is not just about including design materials and documentation. Our way of approaching this issue is by drawing on an age-old method as a way of disseminating knowledge, the apprenticeship. We propose the DMI apprenticeship as a way of exploring the procedural obstacles of replicating DMIs, while highlighting for both apprentice and designer the elements of knowledge that are a challenge to communicate in conventional documentation. Our own engagement with the DMI apprenticeship led to successfully replicating an instrument, Strummi. Framing this process as an apprenticeship highlighted the non-obvious areas of the documentation and manufacturing process that are crucial in the successful replication of a DMI.}
}

@inproceedings{NIME21_27,
  article-number = {27},
  author = {Cotton, Kelsey and Sanches, Pedro and Tsaknaki, Vasiliki and Karpashevich, Pavel},
  title = {The Body Electric: A NIME designed through and with the somatic experience of singing},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2021},
  month = jun,
  address = {Shanghai, China},
  issn = {2220-4806},
  doi = {10.21428/92fbeb44.ec9f8fdd},
  url = {https://nime.pubpub.org/pub/ntm5kbux},
  presentation-video = {https://youtu.be/zwzCgG8MXNA},
  abstract = {This paper presents the soma design process of creating Body Electric: a novel interface for the capture and use of biofeedback signals and physiological changes generated in the body by breathing, during singing. This NIME design is grounded in the performer's experience of, and relationship to, their body and their voice. We show that NIME design using principles from soma design can offer creative opportunities in developing novel sensing mechanisms, which can in turn inform composition and further elicit curious engagements between performer and artefact, disrupting notions of performer-led control. As contributions, this work 1) offers an example of NIME design for situated living, feeling, performing bodies, and 2) presents the rich potential of soma design as a path for designing in this context.}
}

@inproceedings{NIME21_28,
  article-number = {28},
  author = {Frid, Emma and Ilsar, Alon},
  title = {Reimagining (Accessible) Digital Musical Instruments: A Survey on Electronic Music-Making Tools},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2021},
  month = jun,
  address = {Shanghai, China},
  issn = {2220-4806},
  doi = {10.21428/92fbeb44.c37a2370},
  url = {https://nime.pubpub.org/pub/reimaginingadmis},
  presentation-video = {https://youtu.be/vX8B7fQki_w},
  abstract = {This paper discusses findings from a survey on interfaces for making electronic music. We invited electronic music makers of varying experience to reflect on their practice and setup and to imagine and describe their ideal interface for music-making. We also asked them to reflect on the state of gestural controllers, machine learning, and artificial intelligence in their practice. We had 118 people respond to the survey, with 40.68% professional musicians, and 10.17% identifying as living with a disability or access requirement. Results highlight limitations of music-making setups as perceived by electronic music makers, reflections on how imagined novel interfaces could address such limitations, and positive attitudes towards ML and AI in general.}
}

@inproceedings{NIME21_29,
  article-number = {29},
  author = {Pitkin, Jonathan},
  title = {SoftMRP: a Software Emulation of the Magnetic Resonator Piano},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2021},
  month = jun,
  address = {Shanghai, China},
  issn = {2220-4806},
  doi = {10.21428/92fbeb44.9e7da18f},
  url = {https://nime.pubpub.org/pub/m9nhdm0p},
  presentation-video = {https://youtu.be/Fw43nHVyGUg},
  abstract = {The Magnetic Resonator Piano (MRP) is a relatively well-established DMI which significantly expands the capabilities of the acoustic piano. This paper presents SoftMRP, a Max/MSP patch designed to emulate the physical MRP and thereby to allow rehearsal of MRP repertoire and performance techniques using any MIDI keyboard and expression pedal; it is hoped that the development of such a tool will encourage even more widespread adoption of the original instrument amongst composers and performers. This paper explains SoftMRP’s features and limitations, discussing the challenges of approximating responses which rely upon the MRP’s continuous sensing of key position, and considering ways in which the development of the emulation might feed back into the development of the original instrument, both specifically and more broadly: since it was designed by a composer, based on his experience of writing for the instrument, it offers the MRP’s designers an insight into how the instrument is conceptualised and understood by the musicians who use it.}
}

@inproceedings{NIME21_30,
  article-number = {30},
  author = {Tsoukalas, Kyriakos and Bukvic, Ivica},
  title = {Music Computing and Computational Thinking: A Case Study},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2021},
  month = jun,
  address = {Shanghai, China},
  issn = {2220-4806},
  doi = {10.21428/92fbeb44.1eeb3ada},
  url = {https://nime.pubpub.org/pub/t94aq9rf},
  presentation-video = {https://youtu.be/pdsfZX_kJBo},
  abstract = {The NIME community has proposed a variety of interfaces that connect making music and education. This paper reviews current literature, proposes a method for developing educational NIMEs, and reflects on a way to manifest computational thinking through music computing. A case study is presented and discussed in which a programmable mechatronics educational NIME and a virtual simulation of the NIME offered as a web application were developed.}
}

@inproceedings{NIME21_31,
  article-number = {31},
  author = {West, Travis and Caramiaux, Baptiste and Huot, Stéphane and Wanderley, Marcelo M.},
  title = {Making Mappings: Design Criteria for Live Performance},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2021},
  month = jun,
  address = {Shanghai, China},
  issn = {2220-4806},
  doi = {10.21428/92fbeb44.04f0fc35},
  url = {https://nime.pubpub.org/pub/f1ueovwv},
  presentation-video = {https://youtu.be/3hM531E_vlg},
  abstract = {We present new results combining data from a previously published study of the mapping design process and a new replication of the same method with a group of participants having different background expertise. Our thematic analysis of participants' interview responses reveal some design criteria common to both groups of participants: mappings must manage the balance of control between the instrument and the player, and they should be easy to understand for the player and audience. We also consider several criteria that distinguish the two groups' evaluation strategies. We conclude with important discussion of the mapping designer's perspective, performance with gestural controllers, and the difficulties of evaluating mapping designs and musical instruments in general.}
}

@inproceedings{NIME21_32,
  article-number = {32},
  author = {Martelloni, Andrea and McPherson, Andrew and Barthet, Mathieu},
  title = {Guitar augmentation for Percussive Fingerstyle: Combining self-reflexive practice and user-centred design},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2021},
  month = jun,
  address = {Shanghai, China},
  issn = {2220-4806},
  doi = {10.21428/92fbeb44.2f6db6e6},
  url = {https://nime.pubpub.org/pub/zgj85mzv},
  presentation-video = {https://youtu.be/qeX6dUrJURY},
  abstract = {What is the relationship between a musician-designer's auditory imagery for a musical piece, a design idea for an augmented instrument to support the realisation of that piece, and the aspiration to introduce the resulting instrument to a community of like-minded performers? We explore this NIME topic in the context of building the first iteration of an augmented acoustic guitar prototype for percussive fingerstyle guitarists. The first author, himself a percussive fingerstyle player, started the project of an augmented guitar with expectations and assumptions made around his own playing style, and in particular around the arrangement of one song. This input was complemented by the outcome of an interview study, in which percussive guitarists highlighted functional and creative requirements to suit their needs. We ran a pilot study to assess the resulting prototype, involving two other players. We present their feedback on two configurations of the prototype, one equalising the signal of surface sensors and the other based on sample triggering. The equalisation-based setting was better received, however both participants provided useful suggestions to improve the sample-triggering model following their own auditory imagery.}
}

@inproceedings{NIME21_33,
  article-number = {33},
  author = {Nuttall, Thomas and Haki, Behzad and Jorda, Sergi},
  title = {Transformer Neural Networks for Automated Rhythm Generation},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2021},
  month = jun,
  address = {Shanghai, China},
  issn = {2220-4806},
  doi = {10.21428/92fbeb44.fe9a0d82},
  url = {https://nime.pubpub.org/pub/8947fhly},
  presentation-video = {https://youtu.be/Ul9s8qSMUgU},
  abstract = {Recent applications of Transformer neural networks in the field of music have demonstrated their ability to effectively capture and emulate long-term dependencies characteristic of human notions of musicality and creative merit. We propose a novel approach to automated symbolic rhythm generation, where a Transformer-XL model trained on the Magenta Groove MIDI Dataset is used for the tasks of sequence generation and continuation. Hundreds of generations are evaluated using blind-listening tests to determine the extent to which the aspects of rhythm we understand to be valuable are learnt and reproduced. Our model is able to achieve a standard of rhythmic production comparable to human playing across arbitrarily long time periods and multiple playing styles.}
}

@inproceedings{NIME21_34,
  article-number = {34},
  author = {Holzer, Derek and Frisk, Henrik and Holzapfel, Andre},
  title = {Sounds of Futures Passed: Media Archaeology and Design Fiction as NIME Methodologies},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2021},
  month = jun,
  address = {Shanghai, China},
  issn = {2220-4806},
  doi = {10.21428/92fbeb44.2723647f},
  url = {https://nime.pubpub.org/pub/200fpd5a},
  presentation-video = {https://youtu.be/qBapYX7IOHA},
  abstract = {This paper provides a study of a workshop which invited composers, musicians, and sound designers to explore instruments from the history of electronic sound in Sweden. The workshop participants applied media archaeology methods towards analyzing one particular instrument from the past, the Dataton System 3000. They then applied design fiction methods towards imagining several speculative instruments of the future. Each stage of the workshop revealed very specific utopian ideas surrounding the design of sound instruments. After introducing the background and methods of the workshop, the authors present an overview and thematic analysis of the workshop's outcomes. The paper concludes with some reflections on the use of this method-in-progress for investigating the ethics and affordances of historical electronic sound instruments. It also suggests the significance of ethics and affordances for the design of contemporary instruments.}
}

@inproceedings{NIME21_35,
  article-number = {35},
  author = {Regimbal, Juliette and Wanderley, Marcelo M.},
  title = {Interpolating Audio and Haptic Control Spaces},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2021},
  month = jun,
  address = {Shanghai, China},
  issn = {2220-4806},
  doi = {10.21428/92fbeb44.1084cb07},
  url = {https://nime.pubpub.org/pub/zd2z1evu},
  presentation-video = {https://youtu.be/eH3mn1Ad5BE},
  abstract = {Audio and haptic sensations have previously been linked in the development of NIMEs and in other domains like human-computer interaction. Most efforts to work with these modalities together tend to either treat haptics as secondary to audio, or conversely, audio as secondary to haptics, and design sensations in each modality separately. In this paper, we investigate the possibility of designing audio and vibrotactile effects simultaneously by interpolating audio-haptic control spaces. An inverse radial basis function method is used to dynamically create a mapping from a two-dimensional space to a many-dimensional control space for multimodal effects based on user-specified control points. Two proofs of concept were developed focusing on modifying the same structure across modalities and parallel structures.}
}

@inproceedings{NIME21_36,
  article-number = {36},
  author = {Knotts, Shelly},
  title = {Algorithmic Power Ballads},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2021},
  month = jun,
  address = {Shanghai, China},
  issn = {2220-4806},
  doi = {10.21428/92fbeb44.548cca2b},
  url = {https://nime.pubpub.org/pub/w2ubqkv4},
  abstract = {Algorithmic Power Ballads is a performance for Saxophone and autonomous improvisor, with an optional third performer who can use the web interface to hand-write note sequences, and adjust synthesis parameters. The performance system explores shifting power dynamics between acoustic, algorithmic and autonomous performers through modifying the amount of control and agency they have over the sound over the duration of the performance. A higher-level algorithm how strongly the machine listening algorithms, which analyse the saxophone input, influence the rhythmic and melodic patterns generated by the system. The autonomous improvisor is trained on power ballad melodies prior to the performance and in lieu of influence from the saxophonist and live coder strays towards melodic phrases from this musical style. The piece is written in javascript and WebAudio API and uses MMLL a browser-based machine listening library.}
}

@inproceedings{NIME21_37,
  article-number = {37},
  author = {Lee, Myungin},
  title = {Entangled: A Multi-Modal, Multi-User Interactive Instrument in Virtual 3D Space Using the Smartphone for Gesture Control},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2021},
  month = jun,
  address = {Shanghai, China},
  issn = {2220-4806},
  doi = {10.21428/92fbeb44.eae7c23f},
  url = {https://nime.pubpub.org/pub/4gt8wiy0},
  presentation-video = {https://youtu.be/NjpXFYDvuZw},
  abstract = {In this paper, Entangled, a multi-modal instrument in virtual 3D space with sound, graphics, and the smartphone-based gestural interface for multi-user is introduced. Within the same network, the players can use their smartphone as the controller by entering a specific URL into their smartphone’s browser. After joining the network, by actuating the smartphone's accelerometer, the players apply gravitational force to a swarm of particles in the virtual space. Machine learning-based gesture pattern recognition is parallelly used to increase the functionality of the gestural command. Through this interface, the player can achieve intuitive control of gravitation in virtual reality (VR) space. The gravitation becomes the medium of the system involving physics, graphics, and sonification which composes a multimodal compositional language with cross-modal correspondence. Entangled is built on AlloLib, which is a cross-platform suite of C++ components for building interactive multimedia tools and applications. Throughout the script, the reason for each decision is elaborated arguing the importance of crossmodal correspondence in the design procedure.}
}

@inproceedings{NIME21_38,
  article-number = {38},
  author = {Thelle, Notto J. W. and Pasquier, Philippe},
  title = {Spire Muse: A Virtual Musical Partner for Creative Brainstorming},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2021},
  month = jun,
  address = {Shanghai, China},
  issn = {2220-4806},
  doi = {10.21428/92fbeb44.84c0b364},
  url = {https://nime.pubpub.org/pub/wcj8sjee},
  presentation-video = {https://youtu.be/4QMQNyoGfOs},
  abstract = {We present Spire Muse, a co-creative musical agent that engages in different kinds of interactive behaviors. The software utilizes corpora of solo instrumental performances encoded as self-organized maps and outputs slices of the corpora as concatenated, remodeled audio sequences. Transitions between behaviors can be automated, and the interface enables the negotiation of these transitions through feedback buttons that signal approval, force reversions to previous behaviors, or request change. Musical responses are embedded in a pre-trained latent space, emergent in the interaction, and influenced through the weighting of rhythmic, spectral, harmonic, and melodic features. The training and run-time modules utilize a modified version of the MASOM agent architecture. Our model stimulates spontaneous creativity and reduces the need for the user to sustain analytical mind frames, thereby optimizing flow. The agent traverses a system autonomy axis ranging from reactive to proactive, which includes the behaviors of shadowing, mirroring, and coupling. A fourth behavior—negotiation—is emergent from the interface between agent and user. The synergy of corpora, interactive modes, and influences induces musical responses along a musical similarity axis from converging to diverging. We share preliminary observations from experiments with the agent and discuss design challenges and future prospects.}
}

@inproceedings{NIME21_39,
  article-number = {39},
  author = {Leeuw, Hans},
  title = {Virtuoso mapping for the Electrumpet, a hyperinstrument strategy},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2021},
  month = jun,
  address = {Shanghai, China},
  issn = {2220-4806},
  doi = {10.21428/92fbeb44.a8e0cceb},
  url = {https://nime.pubpub.org/pub/fxe52ym6},
  presentation-video = {https://youtu.be/oHM_WfHOGUo},
  abstract = {This paper introduces a new Electrumpet control system that affords for quick and easy access to all its electro-acoustic features. The new implementation uses virtuosic gestures learned on the acoustic trumpet for quick electronic control, showing its effectiveness by controlling an innovative interactive harmoniser. Seamless transition from the smooth but rigid, often uncommunicative sound of the harmoniser to a more noisy, open and chaotic sound world required the addition of extra features and scenarios. This prepares the instrument for multiple musical environments, including free improvised settings with large sonic diversity. The system should particularly interest virtuoso improvising electroacoustic musicians and hyperinstrument player/developers that combine many musical styles in their art and that look for inspiration to use existing virtuosity for electronic control.}
}

@inproceedings{NIME21_40,
  article-number = {40},
  author = {Xambó, Anna and Roma, Gerard and Roig, Sam and Solaz, Eduard},
  title = {Live Coding with the Cloud and a Virtual Agent},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2021},
  month = jun,
  address = {Shanghai, China},
  issn = {2220-4806},
  doi = {10.21428/92fbeb44.64c9f217},
  url = {https://nime.pubpub.org/pub/zpdgg2fg},
  presentation-video = {https://youtu.be/F4UoH1hRMoU},
  abstract = {The use of crowdsourced sounds in live coding can be seen as an example of asynchronous collaboration. It is not uncommon for crowdsourced databases to return unexpected results to the queries submitted by a user. In such a situation, a live coder is likely to require some degree of additional filtering to adapt the results to her/his musical intentions. We refer to this context-dependent decisions as situated musical actions. Here, we present directions for designing a customisable virtual companion to help live coders in their practice. In particular, we introduce a machine learning (ML) model that, based on a set of examples provided by the live coder, filters the crowdsourced sounds retrieved from the Freesound online database at performance time. We evaluated a first illustrative model using objective and subjective measures. We tested a more generic live coding framework in two performances and two workshops, where several ML models have been trained and used. We discuss the promising results for ML in education, live coding practices and the design of future NIMEs.}
}

@inproceedings{NIME21_41,
  article-number = {41},
  author = {Zhang, Yixiao and Xia, Gus and Levy, Mark and Dixon, Simon},
  title = {COSMIC: A Conversational Interface for Human-AI Music Co-Creation},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2021},
  month = jun,
  address = {Shanghai, China},
  issn = {2220-4806},
  doi = {10.21428/92fbeb44.110a7a32},
  url = {https://nime.pubpub.org/pub/in6wsc9t},
  presentation-video = {https://youtu.be/o5YO0ni7sng},
  abstract = {In this paper, we propose COSMIC, a COnverSational Interface for Human-AI MusIc Co-Creation. It is a chatbot with a two-fold design philosophy: to understand human creative intent and to help humans in their creation. The core Natural Language Processing (NLP) module is responsible for three functions: 1) understanding human needs in chat, 2) cross-modal interaction between natural language understanding and music generation models, and 3) mixing and coordinating multiple algorithms to complete the composition.1}
}

@inproceedings{NIME21_42,
  article-number = {42},
  author = {Dublon, Gershon and Liu, Xin},
  title = {Living Sounds: Live Nature Sound as Online Performance Space},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2021},
  month = jun,
  address = {Shanghai, China},
  issn = {2220-4806},
  doi = {10.21428/92fbeb44.b90e0fcb},
  url = {https://nime.pubpub.org/pub/46by9xxn},
  presentation-video = {https://youtu.be/tE4YMDf-bQE},
  abstract = {This paper presents Living Sounds, an internet radio station and online venue hosted by nature. The virtual space is animated by live sound from a restored wetland wildlife sanctuary, spatially mixed from dozens of 24/7 streaming microphones across the landscape. The station’s guests are invited artists and others whose performances are responsive to and contingent upon the ever-changing environmental sound. Subtle, sound-active drawings by different visual designers anchor the one-page website. Using low latency, high fidelity WebRTC, our system allows guests to mix themselves in, remix the raw nature streams, or run our multichannel sources fully through their own processors. Created in early 2020 in response to the locked down conditions of the COVID-19 pandemic, the site became a virtual oasis, with usage data showing long duration visits. In collaboration with several festivals that went online in 2020, programmed live content included music, storytelling, and guided meditation. One festival commissioned a local microphone installation, resulting in a second nature source for the station: 5-channels of sound from a small Maine island. Catalyzed by recent events, when many have been separated from environments of inspiration and restoration, we propose Living Sounds as both a virtual nature space for cohabitation and a new kind of contingent online venue.}
}

@inproceedings{NIME21_43,
  article-number = {43},
  author = {Villicaña-Shaw, Nathan and Carnegie, Dale A. and Murphy, Jim and Zareei, Mo},
  title = {Speculātor: visual soundscape augmentation of natural environments},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2021},
  month = jun,
  address = {Shanghai, China},
  issn = {2220-4806},
  doi = {10.21428/92fbeb44.e521c5a4},
  url = {https://nime.pubpub.org/pub/pxr0grnk},
  presentation-video = {https://youtu.be/kP3fDzAHXDw},
  abstract = {Speculātor is presented as a fist-sized, battery-powered, environmentally aware, soundscape augmentation artifact that listens to the sonic environment and provides real-time illuminated visual feedback in reaction to what it hears. The visual soundscape augmentations these units offer allow for creating sonic art installations whose artistic subject is the unaltered in-situ sonic environment. Speculātor is designed to be quickly installed in exposed outdoor environments without power infrastructure to allow maximum flexibility when selecting exhibition locations. Data from light, temperature, and humidity sensors guide behavior to maximize soundscape augmentation effectiveness and protect artifacts from operating under dangerous environmental conditions. To highlight the music-like qualities of cicada vocalizations, installations conducted between October 2019 and March 2020, where multiple Speculātor units are installed in outdoor natural locations are presented as an initial case study.}
}

@inproceedings{NIME21_44,
  article-number = {44},
  author = {Thompson, William and Berdahl, Edgar},
  title = {An Infinitely Sustaining Piano Achieved Through a Soundboard-Mounted Shaker  },
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2021},
  month = jun,
  address = {Shanghai, China},
  issn = {2220-4806},
  doi = {10.21428/92fbeb44.2c4879f5},
  url = {https://nime.pubpub.org/pub/cde9r70r},
  presentation-video = {https://youtu.be/YRby0VdL8Nk},
  abstract = {This paper outlines a demonstration of an acoustic piano augmentation that allows for infinite sustain of one or many notes. The result is a natural sounding piano sustain that lasts for an unnatural period of time. Using a tactile shaker, a contact microphone and an amplitude activated FFT-freeze Max patch, this system is easily assembled and creates an infinitely sustaining piano.}
}

@inproceedings{NIME21_45,
  article-number = {45},
  author = {Quigley, Michael and Payne, William},
  title = {Toneblocks: Block-based musical programming},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2021},
  month = jun,
  address = {Shanghai, China},
  issn = {2220-4806},
  doi = {10.21428/92fbeb44.46c0f6ef},
  url = {https://nime.pubpub.org/pub/qn6lqnzx},
  presentation-video = {https://youtu.be/c64l1hK3QiY},
  abstract = {Block-based coding environments enable novices to write code that bypasses the syntactic complexities of text. However, we see a lack of effective block-based tools that balance programming with expressive music making. We introduce Toneblocks1, a prototype web application intended to be intuitive and engaging for novice users with interests in computer programming and music. Toneblocks is designed to lower the barrier of entry while increasing the ceiling of expression for advanced users. In Toneblocks, users produce musical loops ranging from static sequences to generative systems, and can manipulate their properties live. Pilot usability tests conducted with two participants provide evidence that the current prototype is easy to use and can produce complex musical output. An evaluation offers potential future improvements including user-defined variables and functions, and rhythmic variability.}
}

@inproceedings{NIME21_46,
  article-number = {46},
  author = {Wu, Yi and Freeman, Jason},
  title = {Ripples: An Auditory Augmented Reality iOS Application for the Atlanta Botanical Garden},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2021},
  month = jun,
  address = {Shanghai, China},
  issn = {2220-4806},
  doi = {10.21428/92fbeb44.b8e82252},
  url = {https://nime.pubpub.org/pub/n1o19efr},
  presentation-video = {https://youtu.be/T7EJVACX3QI},
  abstract = {This paper introduces “Ripples”, an iOS application for the Atlanta Botanical Garden that uses auditory augmented reality to provide an intuitive music guide by seamlessly integrating information about the garden into the visiting experience. For each point of interest nearby, “Ripples” generates music in real time, representing a location through data collected from users’ smartphones. The music is then overlaid onto the physical environment and binaural spatialization indicates real-world coordinates of their represented places. By taking advantage of the human auditory sense’s innate spatial sound source localization and source separation capabilities, “Ripples” makes navigation intuitive and information easy to understand.}
}

@inproceedings{NIME21_47,
  article-number = {47},
  author = {LUCAS, Thomas and d'Alessandro, Christophe and Laubier, Serge de},
  title = {Mono-Replay : a software tool for digitized sound animation},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2021},
  month = jun,
  address = {Shanghai, China},
  issn = {2220-4806},
  doi = {10.21428/92fbeb44.7b843efe},
  url = {https://nime.pubpub.org/pub/8lqitvvq},
  presentation-video = {https://youtu.be/Ck79wRgqXfU},
  abstract = {This article describes Mono-Replay, a software environment designed for sound animation. "Sound animation" in this context means musical performance based on various modes of replay and transformation of all kinds of recorded music samples. Sound animation using Mono-Replay is a two-step process, including an off-line analysis phase and on-line performance or synthesis phase. The analysis phase proceeds with time segmentation, and the set up of anchor points corresponding to temporal musical discourse parameters (notes, pulses, events). This allows, at the performance phase, for control of timing, playback position, playback speed, and a variety of spectral effects, with the help of gesture interfaces. Animation principles and software features of Mono-Replay are described. Two examples of sound animation based on beat tracking and transient detection algorithms are presented (a multi-track record of Superstition by Steve Wonder and Jeff Beck and Accidents/Harmoniques, an electroacoustic piece by Bernard Parmegiani). With the help of these two contrasted examples, the fundamental principles of “sound animation” are reviewed: parameters of musical discourse, audio file segmentation, gestural control and interaction for animation at the performance stage.}
}

@inproceedings{NIME21_48,
  article-number = {48},
  author = {Slager, Ward J.},
  title = {Designing and performing with Pandora’s Box: transforming feedback physically and with algorithms},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2021},
  month = jun,
  address = {Shanghai, China},
  issn = {2220-4806},
  doi = {10.21428/92fbeb44.61b13baf},
  url = {https://nime.pubpub.org/pub/kx6d0553},
  presentation-video = {https://youtu.be/s89Ycd0QkDI},
  abstract = {This paper discusses Pandora's Box, a novel idiosyncratic electroacoustic instrument and performance utilizing feedback as sound generation principle. The instrument's signal path consists of a closed-loop through custom DSP algorithms and a spring. Pandora's Box is played by tactile interaction with the spring and a control panel with faders and switches. The design and implementation are described and rituals are explained referencing a video recording of a concert.}
}

@inproceedings{NIME21_49,
  article-number = {49},
  author = {Chronopoulos, Chris},
  title = {Quadrant: A Multichannel, Time-of-Flight Based Hand Tracking Interface for Computer Music},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2021},
  month = jun,
  address = {Shanghai, China},
  issn = {2220-4806},
  doi = {10.21428/92fbeb44.761367fd},
  url = {https://nime.pubpub.org/pub/quadrant},
  presentation-video = {https://youtu.be/p8flHKv17Y8},
  abstract = {Quadrant is a new human-computer interface based on an array of distance sensors. The hardware consists of 4 time-of-flight detectors and is designed to detect the position, velocity, and orientation of the user's hand in free space. Signal processing is used to recognize gestures and other events, which we map to a variety of musical parameters to demonstrate possible applications. We have developed Quadrant as an open-hardware circuit board, which acts as a USB controller to a host computer.}
}

@inproceedings{NIME21_50,
  article-number = {50},
  author = {Andersson López, Lisa and Svenns, Thelma and Holzapfel, Andre},
  title = {Sensitiv – Designing a Sonic Co-play Tool for Interactive Dance},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2021},
  month = jun,
  address = {Shanghai, China},
  issn = {2220-4806},
  doi = {10.21428/92fbeb44.18c3fc2b},
  url = {https://nime.pubpub.org/pub/y1y5jolp},
  presentation-video = {https://youtu.be/Mo8mVJJrqx8},
  abstract = {In the present study a musician and a dancer explore the co-play between them through sensory technology. The main questions concern the placement and processing of motion sensors, and the choice of sound parameters that a dancer can manipulate. Results indicate that sound parameters of delay and pitch altered dancers’ experience most positively and that placement of sensors on each wrist and ankle with a diagonal mapping of the sound parameters was the most suitable.}
}

@inproceedings{NIME21_51,
  article-number = {51},
  author = {Santos, Geise and Wang, Johnty and Brum, Carolina and Wanderley, Marcelo M. and Tavares, Tiago and Rocha, Anderson},
  title = {Comparative Latency Analysis of Optical and Inertial Motion Capture Systems for Gestural Analysis and Musical Performance},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2021},
  month = jun,
  address = {Shanghai, China},
  issn = {2220-4806},
  doi = {10.21428/92fbeb44.51b1c3a1},
  url = {https://nime.pubpub.org/pub/wmcqkvw1},
  presentation-video = {https://youtu.be/a1TVvr9F7hE},
  abstract = {Wireless sensor-based technologies are becoming increasingly accessible and widely explored in interactive musical performance due to their ubiquity and low-cost, which brings the necessity of understanding the capabilities and limitations of these sensors. This is usually approached by using a reference system, such as an optical motion capture system, to assess the signals’ properties. However, this process raises the issue of synchronizing the signal and the reference data streams, as each sensor is subject to different latency, time drift, reference clocks and initialization timings. This paper presents an empirical quantification of the latency communication stages in a setup consisting of a Qualisys optical motion capture (mocap) system and a wireless microcontroller-based sensor device. We performed event-to-end tests on the critical components of the hybrid setup to determine the synchronization suitability. Overall, further synchronization is viable because of the near individual average latencies of around 25ms for both the mocap system and the wireless sensor interface.}
}

@inproceedings{NIME21_52,
  article-number = {52},
  author = {Portovedo, Henrique and Lopes, Paulo Ferreira and Mendes, Ricardo and Gala, Tiago},
  title = {HASGS: Five Years of Reduced Augmented Evolution},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2021},
  month = jun,
  address = {Shanghai, China},
  issn = {2220-4806},
  doi = {10.21428/92fbeb44.643abd8c},
  url = {https://nime.pubpub.org/pub/1293exfw},
  presentation-video = {https://youtu.be/wRygkMgx2Oc},
  abstract = {The work presented here is based on the Hybrid Augmented Saxophone of Gestural Symbioses (HASGS) system with a focus on and its evolution over the last five years, and an emphasis on its functional structure and the repertoire. The HASGS system was intended to retain focus on the performance of the acoustic instrument, keeping gestures centralised within the habitual practice of the instrument, and reducing the use of external devices to control electronic parameters in mixed music. Taking a reduced approach, the technology chosen to prototype HASGS was developed in order to serve the aesthetic intentions of the pieces being written for it. This strategy proved to avoid an overload of solutions that could bring artefacts and superficial use of the augmentation processes, which sometimes occur on augmented instruments, specially prototyped for improvisational intentionality. Here, we discuss how the repertoire, hardware, and software of the system can be mutually affected by this approach. We understand this project as an empirically-based study which can both serve as a model for analysis, as well provide composers and performers with pathways and creative strategies for the development of augmentation processes.}
}

@inproceedings{NIME21_53,
  article-number = {53},
  author = {Fraisse, Valérian and Guastavino, Catherine and Wanderley, Marcelo M.},
  title = {A Visualization Tool to Explore Interactive Sound Installations},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2021},
  month = jun,
  address = {Shanghai, China},
  issn = {2220-4806},
  doi = {10.21428/92fbeb44.4fd9089c},
  url = {https://nime.pubpub.org/pub/i1rx1t2e},
  presentation-video = {https://youtu.be/MtIVB7P3bs4},
  abstract = {This paper presents a theoretical framework for describing interactive sound installations, along with an interactive database, on a web application, for visualizing various features of sound installations. A corpus of 195 interactive sound installations was reviewed to derive a taxonomy describing them across three perspectives: Artistic Intention, Interaction and System Design. A web application is provided to dynamically visualize and explore the corpus of sound installations using interactive charts (https://isi-database.herokuapp.com/). Our contribution is two-sided: we provide a theoretical framework to characterize interactive sound installations as well as a tool to inform sound artists and designers about up-to-date practices regarding interactive sound installations design.}
}

@inproceedings{NIME21_54,
  article-number = {54},
  author = {Eldridge, Alice and Kiefer, Chris and Overholt, Dan and Ulfarsson, Halldor},
  title = {Self-resonating Vibrotactile Feedback Instruments {\textbar}{\textbar}: Making, Playing, Conceptualising :{\textbar}{\textbar}},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2021},
  month = jun,
  address = {Shanghai, China},
  issn = {2220-4806},
  doi = {10.21428/92fbeb44.1f29a09e},
  url = {https://nime.pubpub.org/pub/6mhrjiqt},
  presentation-video = {https://youtu.be/EP1G4vCVm_E},
  abstract = {Self-resonating vibrotactile instruments (SRIs) are hybrid feedback instruments, characterised by an electro-mechanical feedback loop that is both the means of sound production and the expressive interface. Through the lens of contemporary SRIs, we reflect on how they are characterised, designed, and played. By considering reports from designers and players of this species of instrument-performance system, we explore the experience of playing them. With a view to supporting future research and practice in the field, we illustrate the value of conceptualising SRIs in Cybernetic and systems theoretic terms and suggest that this offers an intuitive, yet powerful basis for future performance, analysis and making; in doing so we close the loop in the making, playing and conceptualisation of SRIs with the aim of nourishing the evolution of theory, creative and technical practice in this field.}
}

@inproceedings{NIME21_55,
  article-number = {55},
  author = {Reynaert, Vincent and Berthaut, Florent and Rekik, Yosra and grisoni, laurent},
  title = {The Effect of Control-Display Ratio on User Experience in Immersive Virtual Musical Instruments},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2021},
  month = jun,
  address = {Shanghai, China},
  issn = {2220-4806},
  doi = {10.21428/92fbeb44.c47be986},
  url = {https://nime.pubpub.org/pub/8n8br4cc},
  presentation-video = {https://youtu.be/d1DthYt8EUw},
  abstract = {Virtual reality (VR) offers novel possibilities of design choices for Digital Musical Instruments in terms of shapes, sizes, sounds or colours, removing many constraints inherent to physical interfaces. In particular, the size and position of the interface components of Immersive Virtual Musical Instruments (IVMIs) can be freely chosen to elicit large or small hand gestures. In addition, VR allows for the manipulation of what users visually perceive of their actual physical actions, through redirections and changes in Control-Display Ratio (CDR). Visual and gestural amplitudes can therefore be defined separately, potentially affecting the user experience in new ways. In this paper, we investigate the use of CDR to enrich the design with a control over the user perceived fatigue, sense of presence and musical expression. Our findings suggest that the CDR has an impact on the sense of presence, on the perceived difficulty of controlling the sound and on the distance covered by the hand. From these results, we derive a set of insights and guidelines for the design of IVMIs.}
}

@inproceedings{NIME21_56,
  article-number = {56},
  author = {Lucas, Alex and Harrison, Jacob and Schroeder, Franziska and Ortiz, Miguel},
  title = {Cross-Pollinating Ecological Perspectives in ADMI Design and Evaluation},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2021},
  month = jun,
  address = {Shanghai, China},
  issn = {2220-4806},
  doi = {10.21428/92fbeb44.ff09de34},
  url = {https://nime.pubpub.org/pub/d72sylsq},
  presentation-video = {https://youtu.be/Khk05vKMrao},
  abstract = {This paper explores ecological perspectives of human activity in the use of digital musical instruments and assistive technology. While such perspectives are relatively nascent in DMI design and evaluation, ecological frameworks have a long-standing foundation in occupational therapy and the design of assistive technology products and services. Informed by two case studies, the authors' critique, compare and marry concepts from each domain to guide future research into accessible music technology. The authors discover that ecological frameworks used by occupational therapists are helpful in describing the nature of individual impairment, disability and situated context. However, such frameworks seemingly flounder when attempting to describe the personal value of music-making.}
}

@inproceedings{NIME21_57,
  article-number = {57},
  author = {Skarha, Matthew and Cusson, Vincent and Frisson, Christian and Wanderley, Marcelo M.},
  title = {Le Bâton: A Digital Musical Instrument Based on the Chaotic Triple Pendulum},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2021},
  month = jun,
  address = {Shanghai, China},
  issn = {2220-4806},
  doi = {10.21428/92fbeb44.09ecc54d},
  url = {https://nime.pubpub.org/pub/uh1zfz1f},
  presentation-video = {https://youtu.be/bLx5b9aqwgI},
  abstract = {This paper describes Le Bâton, a new digital musical instrument based on the nonlinear dynamics of the triple pendulum. The triple pendulum is a simple physical system constructed by attaching three pendulums vertically such that each joint can swing freely. When subjected to large oscillations, its motion is chaotic and is often described as unexpectedly mesmerizing. Le Bâton uses wireless inertial measurement units (IMUs) embedded in each pendulum arm to send real-time motion data to Max/MSP. Additionally, we implemented a control mechanism, allowing a user to remotely interact with it by setting the initial release angle. Here, we explain the motivation and design of Le Bâton and describe mapping strategies. To conclude, we discuss how its nature of user interaction complicates its status as a digital musical instrument.}
}

@inproceedings{NIME21_58,
  article-number = {58},
  author = {Pelofi, Claire and Goldstein, Michal and Bevilacqua, Dana and McPhee, Michael and Abrams, Ellie and Ripollés, Pablo},
  title = {CHILLER: a Computer Human Interface for the Live Labeling of Emotional Responses},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2021},
  month = jun,
  address = {Shanghai, China},
  issn = {2220-4806},
  doi = {10.21428/92fbeb44.5da1ca0b},
  url = {https://nime.pubpub.org/pub/kdahf9fq},
  presentation-video = {https://youtu.be/JujnpqoSdR4},
  abstract = {The CHILLER (a Computer-Human Interface for the Live Labeling of Emotional Responses) is a prototype of an affordable and easy-to-use wearable sensor for the real-time detection and visualization of one of the most accurate biomarkers of musical emotional processing:  the piloerection of the skin (i.e., the goosebumps) that accompany musical chills (also known as musical frissons or shivers down the spine). In controlled laboratory experiments, electrodermal activity (EDA) has been traditionally used to measure fluctuations of musical emotion. EDA is, however, ill-suited for real-world settings (e.g., live concerts) because of its sensitivity to movement, electronic noise and variations in the contact between the skin and the recording electrodes. The CHILLER, based on the Raspberry Pi architecture, overcomes these limitations by using a well-known algorithm capable of detecting goosebumps from a video recording of a patch of skin. The CHILLER has potential applications in both academia and industry and could be used as a tool to broaden participation in STEM, as it brings together concepts from experimental psychology, neuroscience, physiology and computer science in an inexpensive, do-it-yourself device well-suited for educational purposes.}
}

@inproceedings{NIME21_59,
  article-number = {59},
  author = {Lupker, Jeffrey A. T.},
  title = {Score-Transformer: A Deep Learning Aid for Music Composition},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2021},
  month = jun,
  address = {Shanghai, China},
  issn = {2220-4806},
  doi = {10.21428/92fbeb44.21d4fd1f},
  url = {https://nime.pubpub.org/pub/7a6ij1ak},
  presentation-video = {https://youtu.be/CZO8nj6YzVI},
  abstract = {Creating an artificially intelligent (AI) aid for music composers requires a practical and modular approach, one that allows the composer to manipulate the technology when needed in the search for new sounds. Many existing approaches fail to capture the interest of composers as they are limited beyond their demonstrative purposes, allow for only minimal interaction from the composer or require GPU access to generate samples quickly. This paper introduces Score-Transformer (ST), a practical integration of deep learning technology to aid in the creation of new music which works seamlessly alongside any popular software notation (Finale, Sibelius, etc.). Score-Transformer is built upon a variant of the powerful transformer model, currently used in state-of-the-art natural language models. Owing to hierarchical and sequential similarities between music and language, the transformer model can learn to write polyphonic MIDI music based on any styles, genres, or composers it is trained upon. This paper briefly outlines how the model learns and later notates music based upon any prompt given to it from the user. Furthermore, ST can be updated at any time on additional MIDI recordings minimizing the risk of the software becoming outdated or impractical for continued use.}
}

@inproceedings{NIME21_60,
  article-number = {60},
  author = {Michon, Romain and Dumitrascu, Catinca and Chudet, Sandrine and Orlarey, Yann and Letz, Stéphane and Fober, Dominique},
  title = {Amstramgrame: Making Scientific Concepts More Tangible Through Music Technology at School},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2021},
  month = jun,
  address = {Shanghai, China},
  issn = {2220-4806},
  doi = {10.21428/92fbeb44.a84edd3f},
  url = {https://nime.pubpub.org/pub/3zeala6v},
  presentation-video = {https://youtu.be/KTgl4suQ_Ks},
  abstract = {Amstramgrame is a music technology STEAM (Science Technology Engineering Arts and Mathematics) project aiming at making more tangible abstract scientific concepts through the programming of a Digital Musical Instrument (DMI): the Gramophone. Various custom tools ranging from online programming environments to the Gramophone itself have been developed as part of this project. An innovative method anchored in the reality of the field as well as a wide range of key-turn pedagogical scenarios are also part of the Amtramgrame toolkit. This article presents the tools and the method of Amstramgrame as well as the results of its pilot phase. Future directions along with some insights on the implementation of this kind of project are provided as well.}
}

@inproceedings{NIME21_61,
  article-number = {61},
  author = {Reuter, Vivian and Schwarz, Lorenz},
  title = {Wireless Sound Modules},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2021},
  month = jun,
  address = {Shanghai, China},
  issn = {2220-4806},
  doi = {10.21428/92fbeb44.07c72a46},
  url = {https://nime.pubpub.org/pub/muvvx0y5},
  presentation-video = {https://youtu.be/08kfv74Z880},
  abstract = {We study the question of how wireless, self-contained CMOS-synthesizers with built-in speakers can be used to achieve low-threshold operability of multichannel sound fields. We deliberately use low-tech and DIY approaches to build simple sound modules for music interaction and education in order to ensure accessibility of the technology. The modules are operated by wireless power transfer (WPT). A multichannel sound field can be easily generated and modulated by placing several sound objects in proximity to the induction coils. Alterations in sound are caused by repositioning, moving or grouping the sound modules. Although not physically linked to each other, the objects start interacting electro-acoustically when they share the same magnetic field. Because they are equipped with electronic sound generators and transducers, the sound modules can work independently from a sound studio situation.}
}

@inproceedings{NIME21_62,
  article-number = {62},
  author = {Lam, Joshua Ryan and Saitis, Charalampos},
  title = {The Timbre Explorer: A Synthesizer Interface for Educational Purposes and Perceptual Studies},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2021},
  month = jun,
  address = {Shanghai, China},
  issn = {2220-4806},
  doi = {10.21428/92fbeb44.92a95683},
  url = {https://nime.pubpub.org/pub/q5oc20wg},
  presentation-video = {https://youtu.be/EJ0ZAhOdBTw},
  abstract = {When two sounds are played at the same loudness, pitch, and duration, what sets them apart are their timbres. This study documents the design and implementation of the Timbre Explorer, a synthesizer interface based on efforts to dimensionalize this perceptual concept. The resulting prototype controls four perceptually salient dimensions of timbre in real-time: attack time, brightness, spectral flux, and spectral density. A graphical user interface supports user understanding with live visualizations of the effects of each dimension. The applications of this interface are three-fold; further perceptual timbre studies, usage as a practical shortcut for synthesizers, and educating users about the frequency domain, sound synthesis, and the concept of timbre. The project has since been expanded to a standalone version independent of a computer and a purely online web-audio version.}
}

@inproceedings{NIME21_63,
  article-number = {63},
  author = {Svahn, Maria and Hölling, Josefine and Curtsson, Fanny and Nokelainen, Nina},
  title = {The Rullen Band},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2021},
  month = jun,
  address = {Shanghai, China},
  issn = {2220-4806},
  doi = {10.21428/92fbeb44.e795c9b5},
  url = {https://nime.pubpub.org/pub/pvd6davm},
  presentation-video = {https://youtu.be/2cD9f493oJM},
  abstract = {Music education is an important part of the school curriculum; it teaches children to be creative and to collaborate with others. Music gives individuals another medium to communicate through, which is especially important for individuals with cognitive or physical disabilities. Teachers of children with severe disabilities have expressed a lack of musical instruments adapted for these children, which leads to an incomplete music education for this group. This study aims at designing and evaluating a set of collaborative musical instruments for children with cognitive and physical disabilities, and the research is done together with the special education school Rullen in Stockholm, Sweden. The process was divided into three main parts; a pre-study, building and designing, and finally a user study. Based on findings from previous research, together with input received from teachers at Rullen during the pre-study, the resulting design consists of four musical instruments that are connected to a central hub. The results show that the instruments functioned as intended and that the design makes musical learning accessible in a way traditional instruments do not, as well as creates a good basis for a collaborative musical experience. However, fully evaluating the effect of playing together requires more time for the children to get comfortable with the instruments and also for the experiment leaders to test different setups to optimize the conditions for a good interplay.}
}

@inproceedings{NIME21_64,
  article-number = {64},
  author = {Püst, Stefan and Gieseke, Lena and Brennecke, Angela},
  title = {Interaction Taxonomy for Sequencer-Based Music Performances},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2021},
  month = jun,
  address = {Shanghai, China},
  issn = {2220-4806},
  doi = {10.21428/92fbeb44.0d5ab18d},
  url = {https://nime.pubpub.org/pub/gq2ukghi},
  presentation-video = {https://youtu.be/c4MUKWpneg0},
  abstract = {Sequencer-based live performances of electronic music require a variety of interactions. These interactions depend strongly on the affordances and constraints of the used instrument. Musicians may perceive the available interactions offered by the used instrument as limiting. For furthering the development of instruments for live performances and expanding the interaction possibilities, first, a systematic overview of interactions in current sequencer-based music performance is needed.  To that end, we propose a taxonomy of interactions in sequencer-based music performances of electronic music. We identify two performance modes sequencing and sound design and four interaction classes creation, modification, selection, and evaluation. Furthermore, we discuss the influence of the different interaction classes on both, musicians as well as the audience and use the proposed taxonomy to analyze six commercially available hardware devices.}
}

@inproceedings{NIME21_65,
  article-number = {65},
  author = {Corintha, Isabela and Cabral, Giordano},
  title = {Improvised Sound-Making within Musical Apprenticeship and Enactivism: An Intersection between the 4E`s Model and DMIs},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2021},
  month = jun,
  address = {Shanghai, China},
  issn = {2220-4806},
  doi = {10.21428/92fbeb44.56a01d33},
  url = {https://nime.pubpub.org/pub/e4lsrn6c},
  presentation-video = {https://youtu.be/dGb5tl_tA58},
  abstract = {From an epistemological perspective, this work presents a discussion of how the paradigm of enactive music cognition is related to improvisation in the context of the skills and needs of 21st-century music learners. Improvisation in music education is addressed within the perspective of an alternative but an increasingly influential enactive approach to mind (Varela et al., 1993) followed by the four theories known as the 4E of cognition - embedded, embodied, enactive and extended - which naturally have characteristics in common that led them to be grouped in this way. I discuss the “autopoietic” (self-maintain systems that auto-reproduce over time based on their own set of internal rules) nature of the embodied musical mind. To conclude, an overview concerning the enactivist approach within DMIs design in order to provide a better understanding of the experiences and benefits of using new technologies in musical learning contexts is outlined.}
}

@inproceedings{NIME21_66,
  article-number = {66},
  author = {Murray-Browne, Tim and Tigas, Panagiotis},
  title = {Latent Mappings: Generating Open-Ended Expressive Mappings Using Variational Autoencoders},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2021},
  month = jun,
  address = {Shanghai, China},
  issn = {2220-4806},
  doi = {10.21428/92fbeb44.9d4bcd4b},
  url = {https://nime.pubpub.org/pub/latent-mappings},
  presentation-video = {https://youtu.be/zBOHWyIGaYc},
  abstract = {In many contexts, creating mappings for gestural interactions can form part of an artistic process. Creators seeking a mapping that is expressive, novel, and affords them a sense of authorship may not know how to program it up in a signal processing patch. Tools like Wekinator [1] and MIMIC [2] allow creators to use supervised machine learning to learn mappings from example input/output pairings. However, a creator may know a good mapping when they encounter it yet start with little sense of what the inputs or outputs should be. We call this an open-ended mapping process. Addressing this need, we introduce the latent mapping, which leverages the latent space of an unsupervised machine learning algorithm such as a Variational Autoencoder trained on a corpus of unlabelled gestural data from the creator. We illustrate it with Sonified Body, a system mapping full-body movement to sound which we explore in a residency with three dancers.}
}

@inproceedings{NIME21_67,
  article-number = {67},
  author = {Wakefield, Graham},
  title = {A streamlined workflow from Max/gen{\textasciitilde} to modular hardware},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2021},
  month = jun,
  address = {Shanghai, China},
  issn = {2220-4806},
  doi = {10.21428/92fbeb44.e32fde90},
  url = {https://nime.pubpub.org/pub/0u3ruj23},
  presentation-video = {https://youtu.be/xJwI9F9Spbo},
  abstract = {This paper describes Oopsy, which provides a streamlined process for editing digital signal processing algorithms for precise and sample accurate sound generation, transformation and modulation, and placing them in the context of embedded hardware and modular synthesizers. This pipeline gives digital instrument designers the development flexibility of established software with the deployment benefits of working on hardware. Specifically, algorithm design takes place in the flexible context of gen~ in Max, and Oopsy automatically and fluently translates this and uploads it onto the open-ended Daisy embedded hardware. The paper locates this work in the context of related software/hardware workflows, and provides detail of its contributions in design, implementation, and use.}
}

@inproceedings{NIME21_68,
  article-number = {68},
  author = {Dannenberg, Roger B.},
  title = {Canons for Conlon: Composing and Performing Multiple Tempi on the Web},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2021},
  month = jun,
  address = {Shanghai, China},
  issn = {2220-4806},
  doi = {10.21428/92fbeb44.a41fe2c5},
  url = {https://nime.pubpub.org/pub/jxo0v8r7},
  presentation-video = {https://youtu.be/MhcZyE2SCck},
  abstract = {In response to the 2020 pandemic, a new work was composed inspired by the limitations and challenges of performing over the network. Since synchronization is one of the big challenges, or perhaps something to be avoided due to network latency, this work explicitly calls for desynchronization in a controlled way, using metronomes running at different rates to take performers in and out of approximate synchronization. A special editor was developed to visualize the music because conventional editors do not support multiple continuously varying tempi.}
}

@inproceedings{NIME21_69,
  article-number = {69},
  author = {Gioti, Artemi-Maria},
  title = {A Compositional Exploration of Computational Aesthetic Evaluation and AI Bias.},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2021},
  month = jun,
  address = {Shanghai, China},
  issn = {2220-4806},
  doi = {10.21428/92fbeb44.de74b046},
  url = {https://nime.pubpub.org/pub/zpvgmv74},
  presentation-video = {https://youtu.be/9l8NeGmvpDU},
  abstract = {This paper describes a subversive compositional approach to machine learning, focused on the exploration of AI bias and computational aesthetic evaluation. In Bias, for bass clarinet and Interactive Music System, a computer music system using two Neural Networks trained to develop “aesthetic bias” interacts with the musician by evaluating the sound input based on its “subjective” aesthetic judgments. The composition problematizes the discrepancies between the concepts of error and accuracy, associated with supervised machine learning, and aesthetic judgments as inherently subjective and intangible. The methods used in the compositional process are discussed with respect to the objective of balancing the trade-off between musical authorship and interpretative freedom in interactive musical works.}
}

@inproceedings{NIME21_70,
  article-number = {70},
  author = {Sankaranarayanan, Raghavasimhan and Weinberg, Gil},
  title = {Design of Hathaani - A Robotic Violinist for Carnatic Music},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2021},
  month = jun,
  address = {Shanghai, China},
  issn = {2220-4806},
  doi = {10.21428/92fbeb44.0ad83109},
  url = {https://nime.pubpub.org/pub/225tmviw},
  presentation-video = {https://youtu.be/4vNZm2Zewqs},
  abstract = {We present a novel robotic violinist that is designed to play Carnatic music - a music system popular in the southern part of India. The robot plays the D string and uses a single finger mechanism inspired by the Chitravina - a fretless Indian lute. A fingerboard traversal system with a dynamic finger tip apparatus enables the robot to play gamakas - pitch based embellishments in-between notes, which are at the core of Carnatic music. A double roller design is used for bowing which reduces space, produces a tone that resembles the tone of a conventional violin bow, and facilitates super human playing techniques such as infinite bowing. The design also enables the user to change the bow hair tightness to help capture a variety of performing techniques in different musical styles. Objective assessments and subjective listening tests were conducted to evaluate our design, indicating that the robot can play gamakas in a realistic manner and thus, can perform Carnatic music.}
}

@inproceedings{NIME21_71,
  article-number = {71},
  author = {Mills, Damian and Schroeder, Franziska and D'Arcy, John},
  title = {GIVME: Guided Interactions in Virtual Musical Environments: },
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2021},
  month = jun,
  address = {Shanghai, China},
  issn = {2220-4806},
  doi = {10.21428/92fbeb44.5443652c},
  url = {https://nime.pubpub.org/pub/h14o4oit},
  presentation-video = {https://youtu.be/sI0K9sMYc80},
  abstract = {The current generation of commercial hardware and software for virtual reality and immersive environments presents possibilities for a wealth of creative solutions for new musical expression and interaction. This paper explores the affordances of virtual musical environments with the disabled music-making community of Drake Music Project Northern Ireland. Recent collaborations have investigated strategies for Guided Interactions in Virtual Musical Environments (GIVME), a novel concept the authors introduce here. This paper gives some background on disabled music-making with digital musical instruments before sharing recent research projects that facilitate disabled music performance in virtual reality immersive environments. We expand on the premise of GIVME as a potential guideline for musical interaction design for disabled musicians in VR, and take an explorative look at the possibilities and constraints for instrument design for disabled musicians as virtual worlds integrate ever more closely with the real.}
}

@inproceedings{NIME21_72,
  article-number = {72},
  author = {Hege, Anne and Noufi, Camille and Georgieva, Elena and Wang, Ge},
  title = {Instrument Design for The Furies: A LaptOpera},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2021},
  month = jun,
  address = {Shanghai, China},
  issn = {2220-4806},
  doi = {10.21428/92fbeb44.dde5029a},
  url = {https://nime.pubpub.org/pub/gx6klqui},
  presentation-video = {https://youtu.be/QC_-h4cVVog},
  abstract = {In this article, we discuss the creation of The Furies: A LaptOpera, a new opera for laptop orchestra and live vocal soloists that tells the story of the Greek tragedy Electra. We outline the principles that guided our instrument design with the aim of forging direct and visceral connections between the music, the narrative, and the relationship between characters in ways we can simultaneously hear, see, and feel. Through detailed case studies of three instruments—The Rope and BeatPlayer, the tether chorus, and the autonomous speaker orchestra—this paper offers tools and reflections to guide instrument-building in service of narrative-based works through a unified multimedia art form.}
}

@inproceedings{NIME21_73,
  article-number = {73},
  author = {de Jong, Staas},
  title = {Human noise at the fingertip: Positional (non)control under varying haptic × musical conditions},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2021},
  month = jun,
  address = {Shanghai, China},
  issn = {2220-4806},
  doi = {10.21428/92fbeb44.9765f11d},
  url = {https://nime.pubpub.org/pub/bol2r7nr},
  presentation-video = {https://youtu.be/L_WhJ3N-v8c},
  abstract = {As technologies and interfaces for the instrumental control of musical sound get ever better at tracking aspects of human position and motion in space, a fundamental problem emerges: Unintended or even counter-intentional control may result when humans themselves become a source of positional noise. A clear case of what is meant by this, is the “stillness movement” of a body part, occurring despite the simultaneous explicit intention for that body part to remain still. In this paper, we present the results of a randomized, controlled experiment investigating this phenomenon along a vertical axis relative to the human fingertip. The results include characterizations of both the spatial distribution and frequency distribution of the stillness movement observed. Also included are results indicating a possible role for constant forces and viscosities in reducing stillness movement amplitude, thereby potentially enabling the implementation of more positional control of musical sound within the same available spatial range. Importantly, the above is summarized in a form that is directly interpretable for anyone designing technologies, interactions, or performances that involve fingertip control of musical sound. Also, a complete data set of the experimental results is included in the separate Appendices to this paper, again in a format that is directly interpretable.}
}

@inproceedings{NIME21_74,
  article-number = {74},
  author = {Faubel, Christian},
  title = {Emergent Polyrhythmic Patterns with a Neuromorph Electronic Network},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2021},
  month = jun,
  address = {Shanghai, China},
  issn = {2220-4806},
  doi = {10.21428/92fbeb44.e66a8542},
  url = {https://nime.pubpub.org/pub/g04egsqn},
  presentation-video = {https://youtu.be/pJlxVJTMRto},
  abstract = {In this paper I show how it is possible to create polyrhythmic patterns with analogue oscillators by setting up a network of variable resistances that connect these oscillators. The system I present is build with electronic circuits connected to dc-motors and allows for a very tangible and playful exploration of the dynamic properties of artificial neural networks. The theoretical underpinnings of this approach stem from observation and models of synchronization in living organisms, where synchronization and phase-locking is not only an observable phenomenon but can also be seen as a marker of the quality of interaction. Realized as a technical system of analogue oscillators synchronization also appears between oscillators tuned at different basic rhythm and stable polyrhythmic patterns emerge as the result of electrical connections.}
}

@inproceedings{NIME21_75,
  article-number = {75},
  author = {Tragtenberg, João and Albuquerque, Gabriel and Calegario, Filipe},
  title = {Gambiarra and Techno-Vernacular Creativity in NIME Research},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2021},
  month = jun,
  address = {Shanghai, China},
  issn = {2220-4806},
  doi = {10.21428/92fbeb44.98354a15},
  url = {https://nime.pubpub.org/pub/aqm27581},
  presentation-video = {https://youtu.be/iJ8g7vBPFYw},
  abstract = {Over past editions of the NIME Conference, there has been a growing concern towards diversity and inclusion. It is relevant for an international community whose vast majority of its members are in Europe, the USA, and Canada to seek a richer cultural diversity. To contribute to a decolonial perspective in the inclusion of underrepresented countries and ethnic/racial groups, we discuss Gambiarra and Techno-Vernacular Creativity concepts. We believe these concepts may help structure and stimulate individuals from these underrepresented contexts to perform research in the NIME field.}
}

@inproceedings{NIME21_76,
  article-number = {76},
  author = {Roth, Timothy and Huang, Aiyun and Cunningham, Tyler},
  title = {On Parallel Performance Practices: Some Observations on Personalizing DMIs as Percussionists},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2021},
  month = jun,
  address = {Shanghai, China},
  issn = {2220-4806},
  doi = {10.21428/92fbeb44.c61b9546},
  url = {https://nime.pubpub.org/pub/226jlaug},
  presentation-video = {https://youtu.be/kjQDN907FXs},
  abstract = {Digital musical instrument (DMI) design and performance is primarily practiced by those with backgrounds in music technology and human-computer interaction. Research on these topics is rarely led by performers, much less by those without backgrounds in technology. In this study, we explore DMI design and performance from the perspective of a singular community of classically-trained percussionists. We use a practiced-based methodology informed by our skillset as percussionists to study how instrumental skills and sensibilities can be incorporated into the personalization of, and performance with, DMIs. We introduced a simple and adaptable digital musical instrument, built using the Arduino Uno, that individuals (percussionists) could personalize and extend in order to improvise, compose and create music (études). Our analysis maps parallel percussion practices emerging from the resultant DMI compositions and performances by examining the functionality of each Arduino instrument through the lens of material-oriented and communication-oriented approaches to interactivity.}
}

@inproceedings{NIME21_77,
  article-number = {77},
  author = {Yuditskaya, Sofy and Sun, Sophia and Schedel, Margaret},
  title = {Synthetic Erudition Assist Lattice},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2021},
  month = jun,
  address = {Shanghai, China},
  issn = {2220-4806},
  doi = {10.21428/92fbeb44.0282a79c},
  url = {https://nime.pubpub.org/pub/5oupvoun},
  presentation-video = {https://youtu.be/FmTbEUyePXg},
  abstract = {The Seals are a political, feminist, noise, and AI-inspired electronic sorta-surf rock band composed of the talents of Margaret Schedel, Susie Green, Sophia Sun, Ria Rajan, and Sofy Yuditskaya, augmented by the S.E.A.L. (Synthetic Erudition Assist Lattice), as we call the collection of AIs that assist us in creating usable content with which to mold and shape our music and visuals. Our concerts begin by invoking one another through internet conferencing software; during the concert, we play skull augmented theremins while reading GPT2 & GPT3 (Machine Learning language models) generated dialogue over pre-generated songs. As a distributed band we designed our performance to take place over video conferencing systems deliberately incorporating the glitch artifacts that they bring. We use one of the oldest forms of generative operations, throwing dice, as well as the latest in ML technology to create our collaborative music over a distance. In this paper, we illustrate how we leverage the multiple novel interfaces that we use to create our unique sound.}
}

@inproceedings{NIME21_78,
  article-number = {78},
  author = {Blandino, Michael and Berdahl, Edgar},
  title = {Using a Pursuit Tracking Task to Compare Continuous Control of Various NIME Sensors},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2021},
  month = jun,
  address = {Shanghai, China},
  issn = {2220-4806},
  doi = {10.21428/92fbeb44.c2b5a672},
  url = {https://nime.pubpub.org/pub/using-a-pursuit-tracking-task-to-compare-continuous-control-of-various-nime-sensors},
  presentation-video = {https://youtu.be/-p7mp3LFsQg},
  abstract = {This study investigates how accurately users can continuously control a variety of one degree of freedom sensors commonly used in electronic music interfaces. Analysis within an information-theoretic model yields channel capacities of maximum information throughput in bits/sec that can support a unified comparison. The results may inform the design of digital musical instruments and the design of systems with similarly demanding control tasks.}
}

@inproceedings{NIME21_79,
  article-number = {79},
  author = {Schedel, Margaret and Smith, Brian and Cosgrove, Robert and Hwang, Nick},
  title = {RhumbLine: Plectrohyla Exquisita — Spatial Listening of Zoomorphic Musical Robots},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2021},
  month = jun,
  address = {Shanghai, China},
  issn = {2220-4806},
  doi = {10.21428/92fbeb44.9e1312b1},
  url = {https://nime.pubpub.org/pub/f5jtuy87},
  presentation-video = {https://youtu.be/twzpxObh9jw},
  abstract = {Contending with ecosystem silencing in the Anthropocene, RhumbLine: Plectrohyla Exquisita is an installation-scale instrument featuring an ensemble of zoomorphic musical robots that generate an acoustic soundscape from behind an acousmatic veil, highlighting the spatial attributes of acoustic sound. Originally conceived as a physical installation, the global COVID-19 pandemic catalyzed a reconceptualization of the work that allowed it to function remotely and collaboratively with users seeding robotic frog callers with improvised rhythmic calls via the internet—transforming a physical installation into a web-based performable installation-scale instrument. The performed calls from online visitors evolve using AI as they pass through the frog collective. After performing a rhythm, audiences listen ambisonically from behind a virtual veil and attempt to map the formation of the frogs, based on the spatial information embedded in their calls. After listening, audience members can reveal the frogs and their formation. By reconceiving rhumb lines—navigational tools that create paths of constant bearing to navigate space—as sonic tools to spatially orient listeners, RhumbLine: Plectrohyla Exquisita functions as a new interface for spatial musical expression (NISME) in both its physical and virtual instantiations.}
}

@inproceedings{NIME21_80,
  article-number = {80},
  author = {Tahiroğlu, Koray and Kastemaa, Miranda and Koli, Oskar},
  title = {AI-terity 2.0: An Autonomous NIME Featuring GANSpaceSynth Deep Learning Model},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2021},
  month = jun,
  address = {Shanghai, China},
  issn = {2220-4806},
  doi = {10.21428/92fbeb44.3d0e9e12},
  url = {https://nime.pubpub.org/pub/9zu49nu5},
  presentation-video = {https://youtu.be/WVAIPwI-3P8},
  abstract = {In this paper we present the recent developments in the AI-terity instrument. AI-terity is a deformable, non-rigid musical instrument that comprises a particular artificial intelligence (AI) method for generating audio samples for real-time audio synthesis. As an improvement, we developed the control interface structure with additional sensor hardware. In addition, we implemented a new hybrid deep learning architecture, GANSpaceSynth, in which we applied the GANSpace method on the GANSynth model. Following the deep learning model improvement, we developed new autonomous features for the instrument that aim at keeping the musician in an active and uncertain state of exploration. Through these new features, the instrument enables more accurate control on GAN latent space. Further, we intend to investigate the current developments through a musical composition that idiomatically reflects the new autonomous features of the AI-terity instrument. We argue that the present technology of AI is suitable for enabling alternative autonomous features in audio domain for the creative practices of musicians.}
}

@inproceedings{NIME21_81,
  article-number = {81},
  author = {Champagne, Alex and Pritchard, Bob and Dietz, Paul and Fels, Sidney},
  title = {Investigation of a Novel Shape Sensor for Musical Expression},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2021},
  month = jun,
  address = {Shanghai, China},
  issn = {2220-4806},
  doi = {10.21428/92fbeb44.a72b68dd},
  url = {https://nime.pubpub.org/pub/bu2jb1d6},
  presentation-video = {https://youtu.be/CnJmH6fX6XA},
  abstract = {A novel, high-fidelity, shape-sensing technology, BendShape [1], is investigated as an expressive music controller for sound effects, direct sound manipulation, and voice synthesis. Various approaches are considered for developing mapping strategies that create transparent metaphors to facilitate expression for both the performer and the audience. We explore strategies in the input, intermediate, and output mapping layers using a two-step approach guided by Perry’s Principles  [2]. First, we use trial-and-error to establish simple mappings between single input parameter control and effects to identify promising directions for further study. Then, we compose a specific piece that supports different uses of the BendShape mappings in a performance context: this allows us to study a performer trying different types of expressive techniques, enabling us to analyse the role each mapping has in facilitating musical expression. We also investigate the effects these mapping strategies have on performer bandwidth. Our main finding is that the high fidelity of the novel BendShape sensor facilitates creating interpretable input representations to control sound representations, and thereby match interpretations that provide better expressive mappings, such as with vocal shape to vocal sound and bumpiness control; however, direct mappings of individual, independent sensor mappings to effects does not provide obvious advantages over simpler controls. Furthermore, while the BendShape sensor enables rich explorations for sound, the ability to find expressive interpretable shape-to-sound representations while respecting the performer’s bandwidth limitations (caused by having many coupled input degrees of freedom) remains a challenge and an opportunity.}
}

@inproceedings{NIME21_82,
  article-number = {82},
  author = {Robinson, Frederic Anthony},
  title = {Debris: A playful interface for direct manipulation of audio waveforms},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2021},
  month = jun,
  address = {Shanghai, China},
  issn = {2220-4806},
  doi = {10.21428/92fbeb44.02005035},
  url = {https://nime.pubpub.org/pub/xn761337},
  presentation-video = {https://youtu.be/H04LgbZqc-c},
  abstract = {Debris is a playful interface for direct manipulation of audio waveforms. Audio data is represented as a collection of waveform elements, which provide a low-resolution visualisation of the audio sample. Each element, however, can be individually examined, re-positioned, or broken down into smaller fragments, thereby becoming a tangible representation of a moment in the sample. Debris is built around the idea of looking at a sound not as a linear event to be played from beginning to end, but as a non-linear collection of moments, timbres, and sound fragments which can be explored, closely examined and interacted with. This paper positions the work among conceptually related NIME interfaces, details the various user interactions and their mappings and ends with a discussion around the interface’s constraints.}
}

@inproceedings{NIME21_83,
  article-number = {83},
  author = {Gregorio, Jeff and Kim, Youngmoo E.},
  title = {Evaluation of Timbre-Based Control of a Parametric Synthesizer},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2021},
  month = jun,
  address = {Shanghai, China},
  issn = {2220-4806},
  doi = {10.21428/92fbeb44.31419bf9},
  url = {https://nime.pubpub.org/pub/adtb2zl5},
  presentation-video = {https://youtu.be/m7IqWceQmuk},
  abstract = {Musical audio synthesis often requires systems-level knowledge and uniquely analytical approaches to music making, thus a number of machine learning systems have been proposed to replace traditional parameter spaces with more intuitive control spaces based on spatial arrangement of sonic qualities. Some prior evaluations of simplified control spaces have shown increased user efficacy via quantitative metrics in sound design tasks, and some indicate that simplification may lower barriers to entry to synthesis. However, the level and nature of the appeal of simplified interfaces to synthesists merits investigation, particularly in relation to the type of task, prior expertise, and aesthetic values. Toward addressing these unknowns, this work investigates user experience in a sample of 20 musicians with varying degrees of synthesis expertise, and uses a one-week, at-home, multi-task evaluation of a novel instrument presenting a simplified mode of control alongside the full parameter space. We find that our participants generally give primacy to parameter space and seek understanding of parameter-sound relationships, yet most do report finding some creative utility in timbre-space control for discovery of sounds, timbral transposition, and expressive modulations of parameters. Although we find some articulations of particular aesthetic values, relationships to user experience remain difficult to characterize generally.}
}

@inproceedings{NIME21_84,
  article-number = {84},
  author = {Riaño, Milton},
  title = {Hybridization No. 1: Standing at the Boundary between Physical and Virtual Space},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2021},
  month = jun,
  address = {Shanghai, China},
  issn = {2220-4806},
  doi = {10.21428/92fbeb44.d3354ff3},
  url = {https://nime.pubpub.org/pub/h1},
  abstract = {Hybridization No. 1 is a wireless hand-held rotary instrument that allows the performer to simultaneously interact with physical and virtual spaces. The instrument emits visible laser lights and invisible ultrasonic waves which scan the architecture of a physical space. The instrument is also connected to a virtual 3D model of the same space, which allows the performer to create an immersive audiovisual composition that blurs the limits between physical and virtual space. In this paper I describe the instrument, its operation and its integrated multimedia system.}
}

@inproceedings{NIME21_85,
  article-number = {85},
  author = {May, Lloyd and Larsson, Peter},
  title = {Nerve Sensors in Inclusive Musical Performance},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2021},
  month = jun,
  address = {Shanghai, China},
  issn = {2220-4806},
  doi = {10.21428/92fbeb44.82c5626f},
  url = {https://nime.pubpub.org/pub/yxcp36ii},
  presentation-video = {https://youtu.be/qsRVcBl2gAo},
  abstract = {We present the methods and findings of a multi-day performance research lab that evaluated the efficacy of a novel nerve sensor in the context of a physically inclusive performance practice. Nerve sensors are a variant of surface electromyography that are optimized to detect signals from nerve firings rather than skeletal muscle movement, allowing performers with altered muscle physiology or control to use the sensors more effectively. Through iterative co-design and musical performance evaluation, we compared the performative affordances and limitations of the nerve sensor to other contemporary sensor-based gestural instruments. The nerve sensor afforded the communication of gestural effort in a manner that other gestural instruments did not, while offering a smaller palette of reliably classifiable gestures.}
}

@inproceedings{NIME21_86,
  article-number = {86},
  author = {Fernandez, Guadalupe  Babio and Larson, Kent},
  title = {Tune Field},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2021},
  month = jun,
  address = {Shanghai, China},
  issn = {2220-4806},
  doi = {10.21428/92fbeb44.2305755b},
  url = {https://nime.pubpub.org/pub/eqvxspw3},
  presentation-video = {https://youtu.be/2lB8idO_yDs},
  abstract = {This paper introduces Tune Field, a 3-dimensional tangible interface that combines and alters previously existing concepts of topographical, field sensing and capacitive touch interfaces as a method for musical expression and sound visualization. Users are invited to create experimental sound textures while modifying the topography of antennas. The interface’s touch antennas are randomly located on a box promoting exploration and discovery of gesture-to-sound relationships. This way, the interface opens space to playfully producing sound and triggering visuals; thus, converting Tune Field into a sensorial experience.}
}

@inproceedings{NIME21_87,
  article-number = {87},
  author = {Kim, Taejun and Yang, Yi-Hsuan and Nam, Juhan},
  title = {Reverse-Engineering The Transition Regions of Real-World DJ Mixes using Sub-band Analysis with Convex Optimization},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2021},
  month = jun,
  address = {Shanghai, China},
  issn = {2220-4806},
  doi = {10.21428/92fbeb44.4b2fc7b9},
  url = {https://nime.pubpub.org/pub/g7avj1a7},
  presentation-video = {https://youtu.be/ju0P-Zq8Bwo},
  abstract = {The basic role of DJs is creating a seamless sequence of music tracks. In order to make the DJ mix a single continuous audio stream, DJs control various audio effects on a DJ mixer system particularly in the transition region between one track and the next track and modify the audio signals in terms of volume, timbre, tempo, and other musical elements. There have been research efforts to imitate the DJ mixing techniques but they are mainly rule-based approaches based on domain knowledge. In this paper, we propose a method to analyze the DJ mixer control from real-world DJ mixes toward a data-driven approach to imitate the DJ performance. Specifically, we estimate the mixing gain trajectories between the two tracks using sub-band analysis with constrained convex optimization. We evaluate the method by reconstructing the original tracks using the two source tracks and the gain estimate, and show that the proposed method outperforms the linear crossfading as a baseline and the single-band analysis. A listening test from the survey of 14 participants also confirms that our proposed method is superior among them.}
}

@inproceedings{NIME21_88,
  article-number = {88},
  author = {Gaster, Benedict and Challinor, Ryan},
  title = {Bespoke Anywhere},
  booktitle = {Proceedings of the International Conference on New Interfaces for Musical Expression},
  year = {2021},
  month = jun,
  address = {Shanghai, China},
  issn = {2220-4806},
  doi = {10.21428/92fbeb44.02c348fb},
  url = {https://nime.pubpub.org/pub/8jaqbl7m},
  presentation-video = {https://youtu.be/ayJzFVRXPMs},
  abstract = {This paper reports on a project aimed to break away from the portability concerns of native DSP code between different platforms, thus freeing the instrument designer from the burden of porting new Digital Musical Instruments (DMIs) to different architectures. Bespoke Anywhere is a live modular style software DMI with an instance of the Audio Anywhere (AA) framework, that enables working with audio plugins that are compiled once and run anywhere. At the heart of Audio Anywhere is an audio engine whose Digital Signal Processing (DSP) components are written in Faust and deployed with Web Assembly (Wasm). We demonstrate Bespoke Anywhere as a hosting application, for live performance, and music production. We focus on an instance of AA using Faust for DSP, that is statically complied to portable Wasm, and Graphical User Interfaces (GUIs) described in JSON, both of which are loaded dynamically into our modified version of Bespoke.}
}

